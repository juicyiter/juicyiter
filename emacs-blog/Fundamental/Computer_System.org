#+TITLE:    Computer System: A Programming Perspective
#+AUTHOR:   Ethan Mengoreo
#+EMAIL:    mengoreo@163.com
#+STARTUP:  indent
#+LANGUAGE: en
#+OPTIONS:  toc:t num:3
#+SETUPFILE: https://juicyiter.gitee.io/assets/mengoreo.setup

* A tour
*Files* include *text files* which consist exclusively of ASCII
characters and *binary files* (all other files).

#+CAPTION: The compilation system
[[https://juicyiter.gitee.io/images/compile-system.png]]

-  /Preprocessing phase/: Reading the header file and insert it(them)
   into the C program text and generating another C program text
   (=hello.i=).

-  /Compilation phase/: Compiler translate the text file =hello.i= into
   the text file =hello.s=, which contains assembly language program.

   #+BEGIN_QUOTE
   Each statement in an assembly language program *exactly* describes
   *one* low-level machine language instruction in a standard text form.
   Assembly language is useful because it provides a common output
   language for different compilers for different high-level languages.
   For example, C compilers and Fortran compilers both generate output
   files in the same assembly language.
   #+END_QUOTE

-  /Assembly phase/: Next, the assembler translates =hello.s= into
   machine language instructions, packages the in a form known as a
   /relocatable object program/, and stores the result in the object
   file =hello.o=.
   #+BEGIN_QUOTE
   The =hello.o= file is a *binary file* whose bytes encode machine
   language instructions rather than characters.
   #+END_QUOTE

- /Linking phase/: The linker merges the =printf.o= and =hello.o= into
   =hello= or =hello.exe=, which is an /executable object file/(or
   simply /executable/) that is ready to be loaded into memory and
   executed by the system. Object code is one form of machine code, but
   the address of global values are not yer filled in.
   #+BEGIN_QUOTE
   The hello program calls the function =printf=, *which is a part of the
   /standard C library/ provided by every C compiler.* The =printf=
   function *resides in a separate precompiled object file* called
   =printf.o=, which must somehow be merged with =hello.o= program.
   The GNU project

   *GNU*(short for GNU's Not Unix) environment includes the EMACS editor,
   GCC compiler, GDB debugger, assembler, linker, utilities for
   manipulating binaries, and other components.
   #+END_QUOTE

If the *first word* of the command line does not correspond to built-in
shell command, then the shell will take it *as an executable file* and
assume it should be load and run.

*Busses* carries information back and forth between the components and
are typically designed to transfer *fixed-sized* chunks of bytes known
as /words/.

#+BEGIN_QUOTE
  Word size varies across systems. Most machines today have word sizes
  of either 4 bytes(32bits) or 8 bytes(64bits).
#+END_QUOTE

*I/O devices* are the system's connection to the external world. Each
*I/O device* is connected to the I/O bus by either a /controller/ or
/adapter/. The main distinction between them is one of packaging.
/Controllers/ are chip sets in the device itself or on the system's
motherboard. An /adapter/ is a card that plugs into a slot on the
motherboard.

*Main Memory*, which physically consists of a series of DRAM(dynamic
random access memory) chips, is a /temporary/ storage device that holds
both a program and the data it manipulates while the processor is
executing the program.

The central processing unit(*CPU*) or simply processor, is the engine
that *interprets instruction* that stored in main memory. At its core is
a *word-sized storage device* (or *register*) called the program
counter(*PC*). While the system is running, the processor repeatedly
interprets(or execute) instructions pointed by the PC and then updates
PC to point to new instructions.

There are only a few operations, and they revolve around the main
memory, the register file and the arithmetic/logic unit(ALU). The
/register file/ is a *small storage device* the consits of a collection
of *word-sized registers*. each with its own unique name. The ALU
computes new data and address values.

-  /Load/: *Copy* a byte or a word from *main memory into a register*,
   *overwriting* the previous contents of the register.
-  /Store/: *Copy* a byte or a word form *a register to a location in
   main memory*, *overwriting* the previous contents of that location.
-  /Operate/: *Copy* the contents of two *registers to the ALU*, perform
   an arithmetic operation on the two words, and store the result in a
   register, *overwriting* the previous contents of that register.
-  /Jump/: *Extract* a word from the instruction itself and *copy* that
   *word into* the program counter(*PC*), *overwriting* the previous
   value of the PC.

*An important physical laws*: large storage devices are slower than the
small storage devices.

https://juicyiter.gitee.io/images/os-hierarchy.png

From the top of memory hierarchy table to the bottom, the storage
devices become larger, slower, and less costly per byte. The L1, L2 and
L3 cache are implemented with a hardware technology known as */static
random access memory/*. The idea behind caching is that a system can get
the effect of both a very large memory and a very fast one by exploit
/locality/, the tendency off programs to access data and code in
localised regions.

-  CPU registers hold *words* retrieved from L1 cache.
-  L1 cache holds *cache lines* retrieved from L2 cache.
-  L2 cache holds *cache lines* retrieved from L3 cache.
-  L3 cache holds *cache lines* retrieved from Main memory.
-  Main memory holds *disk block* retrieved from local disks.
-  Local disks hold *files* retrieved from disks on remote network
   server.

Files are abstractions for I/O devices. A *file* is a sequence of bytes,
/nothing more and nothing less/. Every I/O device, including disks,
keyboards, displays, and even networks is modelled as a file. This
elegant notation provides applications with a uniform view of all of the
varied I/O devices. All input and output in the system is performed by
reading and writing files, using a small set of systems calls known as
/Unix I/O/.

A process is the operating system's abstraction for a running program.
Multiple processes can run *concurrently* on the same system.By
*concurrently*, we mean that the instructions of one process is
interleaved with instructions of another process.

#+BEGIN_QUOTE
  The operating system performs this interleaving with a mechanism known
  as *context switching* by *saving the context*, which includes
  information such as the current values of the PC, the register file
  and the contents of main memory, of the current process, *restoring
  the context* of the new process, and then *passing control* to the new
  process. The new process picks up exactly where it left off.
#+END_QUOTE

Basically, in a modern system a process can actually consist of multiple
execution units, called *threads*, each running in the context of the
process and sharing the same code and global data.

#+BEGIN_QUOTE
  It's easier to share data between threads than processes and typically
  more efficient than processes. *Multi-threading* is one way to make
  program run faster when multiple processors are available.
#+END_QUOTE

Virtual memory is an abstraction that provides each process with the
*illusion* that it has exclusive use of the main memory.

#+CAPTION: The virtual address space.
[[https://juicyiter.gitee.io/images/virtual-space.png]]

In Linux, the *topmost region* of the address space is reserved for the
code and data in the *operating system* that is common to all processes.
The *lower region* of the address space holds the code and data *defined
by the user's process*.

#+BEGIN_QUOTE
  Addresses in the figure increase from the bottom to top.
#+END_QUOTE

-  */Program code and data./* Code *begins at the same fixed address*
   for all processes, followed by data locations that correspond to
   *global C variables*. The code and data areas are initialised
   directly from the contents of an executable object file, in our case
   the =hello= executable.

-  */Heap./* The code data areas are followed immediately by the
   /run-time/ heap. Unlike the code and data areas which are fixed in
   size, the heap *expands and contract dynamically* at run time as a
   result of calls of C standard library routines such as =malloc= and
   free.

-  */Shared libraries./* Near the middle of the address space is an area
   that holds the code and data for /shared libraries/ such as the C
   standard library and the math library.

-  */Stack./* At the top of the virtual address space is the /user
   stack/ that *the compilers uses to implement functions calls*. Like
   the heap, the user stack *expands and contracts dynamically*. In
   particular, it grows when we call a function and contracts when we
   return from a function.

-  */Kernel virtual memory./* The kernel is the part of the operation
   system that is *always resident in memory.* The top region is
   *reserved* for the kernel. *Applications are not allowed to read or
   write the contents of this area or directly call functions defined in
   the kernel code.*

#+BEGIN_QUOTE
  The operating system kernel serves as an intermediary between the
  applications and the hardware. It provides three fundamental
  abstractions:

  1) Files are abstractions for I/O devices.

  2) Virtual memory is an abstraction for both main memory and disks.

  3) Processes are abstractions for the processor, main memory, and I/O
     devices.

#+END_QUOTE

*CONCURRENCY & PARALLELISM*

The term of concurrency is used to refer to the general concept of a
system with multiple, simultaneous activities and the term of
parallelism is used to refer the use of concurrency to make a system run
faster. Parallelism can be exploited at multiple levels of abstractions
in a computer system. Three levels are highlighted here, working form
the highest to the lowest level in the system hierarchy.

-  Thread-level Concurrency

With concurrency, multiple programs can be executed at the same time.
But, traditionally, this can be only *simulated*, by having a single
computer /rapidly/ switch among its executing processes, much as a
juggler keeps multiple balls flying through the air. Until recently,
most actual computing was done by a configuration is known as a
/uniprocessor system/. With the advent of the /multi-core processor/ and
/hyper-threading/.

Multi-core processors have several CPUs (referred as "cores") integrated
onto a single integrated-circuit chip, *each* of which *with its own L1
and L2 cache but sharing the higher levels of caches as well as the
interface to main memory.*

Hyper-threading, sometimes called /simultaneous multi-threading/, is a
technique that allows a single CPU to execute multiple flows of control.
It involves having *multiple copies* of some of the CPU hardware, such
as *program counters and register files*, while having only *single
copies* of other parts of the hardware, such as the *units that perform
floating-point arithmetic.*

A hyper-threaded processor enables the CPU to make better advantage of
its processing resources. The Intel Core i7 processor can have each cor
executing to threads, and so a four-core system can actually execute
eight threads in parallel.

The use of multi-processing can improve system performance in *two
ways*. First, it reduces the need to simulate concurrency when
performing *multiple* tasks. Second, it can run a single application
program *faster*.

-  Instruction-Level Parallelism

At a much lower level of abstraction, modern processors can execute
multiple instructions at one time, a property known as
/instruction-level parallelism/. In *pipelining*, the actions required
to execute an instruction are *partitioned into different steps* and the
processor hardware is organised as *a series of stages*, each performing
one of these steps.

Processors that can sustain execution rates faster than one instruction
per clock cycle are known as /superscalar/.

-  Single-Instruction, Multiple Data(SIMD) Parallelism

At the lowest level, many modern processors have special hardware that
allows a single instruction to cause multiple operations to be performed
in parallel, which is known as SIMD parallelism. These SIMD instructions
are provided mostly to speed up applications that process images, video
and sound data.


[[https://juicyiter.gitee.io/images/abstractions-of-computer.png]]
* Representing and Manipulating Information

** Information Storage

A machine level program views memory as a large array of bytes, referred
as */virtual memory/*. Each byte of memory is defined by a unique
number, known as its address, and the set of all possible addresses is
known as the */virtual address space/*.

The value of a pointer in C is the virtual address of the *first byte*
of some block of storage.

Every computer has a *word size*, indicating *the nominal size of
integer and pointer data*. Since a virtual address is encoded by such a
word, the most important parameter in computer system determined by such
a word is the maximum size of the virtual address space. That is, if a
computer has a ùë§-bit word size, the virtual address of it can arrange
from 0 ~ 2^{ùë§}Ôºç1, giving the program access at most 2^{ùë§} bytes.

For most computer with a 32-bit word size, the limits of its virtual
space is to 4 Gigabytes (4 GB), that is, *just over* 4 √ó 10^{9} bytes.

#+CAPTION: sizes of data types
https://juicyiter.gitee.io/images/sizes-of-data-types.png

#+BEGIN_QUOTE
  The C data type =char= represents a singe byte. A =long int= uses the
  full word size of the machine. Unsigned numbers obey the law of
  arithmetic modulo 2^{ùëõ}, where ùëõ is the number of bits in the type.
  So, for instance, if =short= is 16 bits, then =unsigned short=
  variables have the value between =0= and =65535=.

  The =long long= type, so as the =char *= type, allows the full range
  of the 64-bit integers. *For 32-bit machines, the compiler must
  compile operations for this data type by generating code that performs
  sequences of 32-bit operations.*
#+END_QUOTE

#+CAPTION: bi-endian
https://juicyiter.gitee.io/images/bi-endian.png

#+BEGIN_QUOTE
  Note that in the word =0x01234567=, the high-order byte has
  hexadecimal value =0x01=, while the low-order has hexadecimal value
  =0x67=.
#+END_QUOTE

#+BEGIN_SRC C
    /* Code to print hte byte representation of program objects */
    /* This code uses casting to circumvent the type system, accessing and printing the byte representations of the program object */

    #include <stdio.h>

    typedef unsigned char *byte_pointer;

    void show_bytes(byte_pointer start, int len)
    {
        int i;
        for(i = 0; i < len; i++)
            printf("%.2x",start[i]);
        printf("\n");
    }

    void show_int(int x)
    {
        show_bytes((byte_pointer)&x, sizeof(int));
    }

    void show_float(float x)
    {
        show_bytes((byte_pointer)&x, sizeof(float));
    }

    void show_pointer(void *x)
    {   //data type void is a special kind of pointer with no associcated type information.
        show_bytes((byte_pointer)x, sizeof(void *));
    }
#+END_SRC

As ASCII has its own character code, independent of the byte ordering
and word size conventions, text data is more platform-independent than
binary data.

Generally, machines support two ways of right shift operation: */logical
and arithmetic/*. A logical right shift fills the left end with ùëò zeros
and an arithmetic right shift with ùëò repetitions of the *most
significant bit*. The C standards don't precisely define which type of
right shift should be used. *For signed data, right shift must be
logical, and for signed data, either logical or arithmetic shifts may be
used*. In practise, however, *the most of the compiler/machine
combinations use arithmetic right shifts for signed data*.

** Integer Representations

Casting the signed number to unsigned number in the same bit size or
vice versa is to keep the bit values identical but change how these bits
are interpreted.

When an operation (or comparison) is performed where both signed and
unsigned operands are involved, *C implicitly cast the signed operand to
unsigned* and performs the operation assuming the numbers are
nonnegative.

#+BEGIN_QUOTE
  *INTERESTING ASIDE*

  When writing */TMin/* in C, we carefully wrote the value of
  */TMin_{32}/* as =-2147483647Ôºç1=. Looking at the header file
  =limits.h=, we see that they use a similar method to write /TMin_{32}/
  and /TMax_{32}/:
#+END_QUOTE

#+BEGIN_SRC C
    #define INT_TMAX 2147483647
    #define INT_TMIN (-INT_TMAX - 1)
#+END_SRC

#+BEGIN_QUOTE
  A curious interaction between the asymmetry of the two's complement
  representation and the convention rules of C force us to write
  /TMin_{32}/ in this unusual way.

  #+CAPTION: TMin test
  https://juicyiter.gitee.io/images/tmin-test.png

  When the compiler encounters a number of the form =‚àíX=, it *first
  determines the data type and value for =X= and then negates it*. The
  value =2147483648= is too large to represent as an =int=, since this
  value is one larger than /TMax_{32}/ (the asymmetry strikes!). The
  compiler tries to determine a data type that can represent this value
  properly. It proceeds down one of the lists shown for the decimal
  cases in the figure below.

  #+CAPTION: Data Orders
  https://juicyiter.gitee.io/images/data-orders.png

  Now we see why =‚àí2147483648= won't do the trick.
#+END_QUOTE

One point worth making is that the relative order of conversion between
one data size to another and between unsigned and signed can affect the
behaviour of a program.

#+BEGIN_SRC C
    short sx = -12345;
    unsigned uy = sx;

    printf("sx = %d:\t", sx);
    show_bytes((byte_pointer)&sx, sizeof(unsigned));
    printf("uy = %u:\t", uy);
    show_bytes((byte_pointer)&uy, sizeof(unsigned));
#+END_SRC

When run on a big-endian machine, this code causes the following output
to be printed:

#+BEGIN_SRC C
    sx = -12345:    cfc7
    uy = 4294954951:    ffffcfc7
#+END_SRC

This shows us that when converting from =short= to =unsigned=, we first
change the size and then from =signed= to =unsigned=. That is,
=(unsigned)sx= is equivalent to =(unsigned)(int)sx=, not
=(unsigned)(unsigned short)sx=.

** Integer Arithmetic

*** Unsigned Addition

Unsigned arithmetic can be viewed as a form of modular arithmetic.
Unsigned addition is equivalent to computing the sum modulo /2^{ùë§}/.
This value can be computed by simply discarding the high-order bit in
the ùë§Ôºã1-bit representation in ùë•Ôºãùë¶.

#+CAPTION: Unsigned
[[https://juicyiter.gitee.io/images/unsigned-addition.png]]

When executing C programs, overflows are not signalled as errors.

#+BEGIN_SRC C
    /* Determin whether arguments can be added without overflow */
    int uadd_ok(unsigned a, unsigned b)
    {
        unsigned sum = a + b;
        return sum >= a;
    }
#+END_SRC

If sum did not overflow, surely ùë†ùë¢ùëö ‚â• ùëé. If sum did overflow, then ùë†ùë¢ùëö =
ùë•Ôºãùë¶Ôºç2^{ùë§}, which ùë¶Ôºç2^{ùë§}Ôºú0, so ùë†ùë¢ùëöÔºúùë•;

*** Unsigned Negation

#+CAPTION: Unsigned Negation
 https://juicyiter.gitee.io/images/unsigned-negation.png

*** Unsigned Multiplication

Unsigned multiplication operation can be viewed to be equivalent to
computing the product modulo 2^{ùë§}, which yields the ùë§-bit value given
by the low-order ùë§ bits of the product.

*** Two's Complement Additon

Observe that the result can also be obtained by performing binary
addition of the operands and truncating the result to w bits.

#+CAPTION: Two's Complement Addition
 https://juicyiter.gitee.io/images/two-s-complement-addition.png

#+BEGIN_SRC C
    /* Determing whether arguments can be added without overflow */

    int tadd_ok(int x, int y)
    {
        int sum = x+y;
        int pos_over = x >= 0 && y >= 0 && sum < 0;
        int neg_over = x < 0 && y < 0 && sum >= 0;
        return !pos_over && !neg_over;
    }

    /* WARING: This is a buggy code */
    int tadd_ok_buggy(int x, int y)
    {
        int sum = x+y;
        return (sum-x == y)&&(sum-y == x);
        /* Since two's complement addition forms an abelian group, and so the expression (x+y)-x will always evaluate to y regardless of whether or not the addition overflows. */
    }
#+END_SRC

*** Two's Complement Negation

 https://juicyiter.gitee.io/images/two-s-complement-negation.png


*** Two's Complement Multiplication

Signed multiplication in C generally is performed by truncation the
2ùë§-bit product to ùë§ bits.We claim that the bit-level representation of
the product operation is identical for both unsigned and two's
complement multiplication.

#+BEGIN_SRC C
    /* Determine whether arguments can be multiplied without overflow */
    int tmult_ok(int x, int y)
    {
        int p = x*y;
        /* Either x is zero, or dividing p by x gives y */
        return !x || p/x == y;
    }
    /* Determine whether arguments can be multiplied without overflow without using dividing */
    int tmult_ok_nodivi(int x, int y)
    {
        /* Computer product without overflow */
        long long pll = (long long)x*y;
        /* See if casting to int preseves value */
        return pll == (int)pll;
    }
#+END_SRC

*** Multiplying by Constants

On most machines, the integer multiply instruction is fairly slow,
requiring 10 or more clock cycles, whereas other integer
operations---such as addition, subtraction, bit-level operations, and
shifting---require only 1 clock cycle.

For an unsigned or signed variable ùë•, the C expression =x<<k= is
equivalent to =x * pwr2k=, where =pwr2k= equals 2^{ùëò}.

#+BEGIN_QUOTE
  Multiplying by a power of 2 can cause overflow with either unsigned or
  two's complement arithmetic. Still, we will get the same effect by
  shifting.
#+END_QUOTE

14 can be written as =(0...0)(111)(0)=. Consider a run of ones from bit
position ùëõ down to bit position ùëö (ùëõ ‚â• ùëö). We can compute the effect of
these bits on the product using either of two different forms:

‚Äã *Form A:* =(x<<n) + (x<<n-1) + ... + (x<<m)=

‚Äã *Form B:* =(x<<n+1) - (x<<m)=

When the ùëõ is the most significant bit in *Form B*, it simply becomes
=-(x<<m)=.

Some special cases:

-  =55= has the bit representation of =110111=, so =x * 55= can be
   computed by =(x<<6) - (x<<0) - (x<<3)=. (We can view this bit pattern
   as having a run of 6 ones with a zero in the middle, and so we apply
   the rule of Form B, but then *subtract the term corresponding to the
   middle zero bit*);
-  =-6= has the bit representation of =1010=, so =x * (-6)= can be
   computed by =-(x<<3)+(x<<1)=. (Where the most significant bit is the
   signed bit).

*** Dividing by Powers of Two

Integer division requires more than 30 or more clock cycles on most
moachines. Dividing by powers of two can be performed by a right shift
operarion. The two different shifts---logical and arithmetic---serve
this purpose for unsigned and two's complement numbers, respectively.

*Integer division always rounds toward zero*. For ùë• ‚â• 0 and ùë¶ ‚â• 0, the
result should be ‚é£ùë•/ùë¶‚é¶. ‚é£Ôºç3.14‚é¶= Ôºç4.

Applying a logical right shift by ùëò to an unsigned number gives the same
results dividing by 2^{ùëò}.

For a positive number, we have 0 as the most significant bit, and so the
effect is the same as for a logical right shift. Thus, an arithmetic
right shift by ùëò is the same for a nonnegative number.

But, for a *negative number*, only right shifting won't gives us what we
want. We need to *add a bias* before the right shift *in order to make
the result round towards zero*. For a two's complement machine using
arithmetic right shifts, the C expression =(x<0 ? x+(1<<k)-1 : x) >> k=
is equivalent to =x / pwr2k=, where =pwr2k= equals 2^{ùëò}.

#+BEGIN_SRC C
    //INTERESTING FUNCTION!!
    /*This function returns the value x/16 for integer argument x, which uses a two's complement representation. */
    /* Right shifts are performed arithmetically */
    int div16(int x)
    {
        /* Compute bias to be either 0 (x >= 0) or 15 (x < 0) */
        int bias = (x >>31) & 0xF;
        return (x + bias) >> 4;
    }
#+END_SRC

*** Summary

When casting between signed and unsigned integers of the same size, most
C implementations follow the convention that the underlying bit pattern
does not change.

Both unsigned and two's-complement arithmetic satisfy many of the other
properties of integer arithmetic, including associativity,
commutativity, and distributivity.

Bit patterns, like =[0,...,0,1,...,1]=, consisting of ùë§Ôºçùëò zeros
followed by k ones, are useful for masking operations. This patter can
be generated by the C expression =(1<<k)-1=, exploiting the property
that the desired bit patter has numeric value 2^{ùëò}-1.

** Floating Point

*** Fractional Binary Numbers

Fractional binary numbers have the form ùëè_{ùëö} ùëè_{ùëöÔºç1} ¬∑¬∑¬∑ ùëè_{1} ùëè_{0} .
ùëè_{Ôºç1} ùëè_{Ôºç2} ¬∑¬∑¬∑ ùëè_{Ôºçùëõ}. This notation represents a number ùëè defined
as:

#+CAPTION: Fractional binary number
 https://juicyiter.gitee.io/images/fractional-binary-number.png


Fractional binary number can only represent numbers that can be written
ùë• √ó 2^{ùë¶}. *Other values can only be approximated*. We increase accuracy
by lengthening the binary representation. For example, the number 1/5
can be represented exactly as the decimal number 0.20. As a fractional
binary number, however, it can't be represented exactly, but only
approximated.

#+CAPTION: Approximated number
https://juicyiter.gitee.io/images/approximated-number.png


*** IEEE Floating-Point Representation

The IEEE (The Institute of Electrical and Electronic Engineers)
floating-point standard represents a number in a form Ôº∂ = (-1)^{ùë†} √ó Ôº≠
√ó 2^{Ôº•} :

-  The /sign/ ùë† is the sign bit. 0 is the special case.
-  The /significand/ Ôº≠ is between 1 to 2Ôºçùúñ or 0 to 1Ôºçùúñ.
-  The /exponent/ Ôº• is what it is.

The bit representation of a floating-point number is divided into three
fields to encode these values:

-  The single sing bit ùë† is directly encodes the sign ùë†.

-  The ùëò-bit exponent field ùëíùë•ùëù = ùëí_{ùëòÔºç1} ¬∑¬∑¬∑ ùëí_{1} ùëí_{0} encodes the
   exponent Ôº•.

-  The ùëõ-bit fraction field ùëìùëüùëéùëê = ùëì_{ùëõÔºç1} ¬∑¬∑¬∑ ùëì_{1} ùëì_{0} encodes the
   significand Ôº≠. But the value encoded also depends on whether or not
   the exponent field equals 0.

#+caption: IEEE Floating-Point Representation
https://juicyiter.gitee.io/images/IEEE-floating-point-representation.png


**** CASE 1: Normalised Values.

-  BiasÔºù2^{ùëòÔºç1}Ôºç1. Ôº•Ôºù ùëíÔºçBias;
-  Ôº≠Ôºù1Ôºãùëì;

**** CASE 2: Denormalised Values.

-  BiasÔºù2^{ùëòÔºç1}Ôºç1. EÔºù1ÔºçBias.
-  MÔºùùëì;
-  Ôºã0(ùë†Ôºù0).
-  Ôºç0(ùë†Ôºù1).

**** CASE 3: Special Values.

-  Infinity and /NaN/ ( Not a Number).

The representable numbers are not uniformly distributed---they are
denser nearer the origin.

The smooth transition between the largest denormalised number and the
smallest normalised number is the result of making the Ôº≠ 1ÔºçBias rather
than ÔºçBias.

*** Rounding

Four rounding models:

-  *Round-to-even*

For those rounding values that are halfway of two possible results. It
will round upward about 50% of the time and downward about 50% of the
time. For example, we will round both =1.235000= and =1.245000= to 1.24,
since 4 is even. When it comes to binary fractional number, those values
that are halfway between to possible results are rounded to the result
whose least significant bit is zero.

-  Round-towards-zero

-  Round-up

-  Round-down

*** Floating-Point Operations

$$\frac{1}{-0}=-\infty \\ \frac{1}{+0}=+\infty$$

The results should be rounded. The operations are commutative but not
associative. The lack of associativity in floating-point addition is the
most important group property that is lacking.

Floating-point addition satisfies the following monotonicity: if ùëé ‚â• b
then ùë•Ôºãùëé ‚â• ùë•Ôºãùëè for any values of ùëé, ùëè, and ùë• other than /NaN/.
Floating-point multiplication satisfies the monotonicity but does not
distribute over addition;

None of these monotonicity properties hold for unsigned or two's
complement multiplicaiton;

*** Floating Point in C

The GNU compiler GCC defines program constants =INFINITY= (for ‚àû) and
=NAN= (for /NAN/) when the following sequence occurs in the program
file:

#+BEGIN_SRC C
    #define GNU SOURCE 1
    #include <math.h>
#+END_SRC

When casting values between =int=, =float=, and =double= formats, the
program changes the numeric values and the bit representations as
follows (assuming a 32-bit =int=).

-  From =int= to =float= (same bytes), the number cannot overflow, but
   it may be rounded.
-  From =int= or =float= to =double=, the exact numeric value can be
   preserved because the =double= has both greater range and greater
   precision.
-  From =double= to =float=, the value can overflow $+\infty$ or $-\infty$, since the range is smaller. Otherwise, it may be rounded, because the precision is smaller.
-  From =float= or =double= to =int= the value will be *rounded toward
   zero*. For instance, 1.999 will be converted to 1, while Ôºç1.999 will
   be converted to Ôºç1. Furthermore, the value may overflow. *Any   conversion from floating point to integer that cannot assign a
   reasonable integer approximation yields /TMin_{w}/.* Thus, the
   expression =(int) +1e10= yields =Ôºç21483648=, generating a negative
   value from a positive one.

#+BEGIN_QUOTE
  *Intel IA32 Floating-Point Arithmetic*

  IA32 processors, like most other processors, have special memory
  elements called /registers/ for holding floating point values as they
  are being computed and used. *The unusual feature of IA32 is that the floating-point registers use a special 80-bit /extended-precision/  format to provide a greater range and precision then the normal 32-bit  single-precision and 64-bit double-precision formats used for values  held in memory.* All single- and double-precision numbers *are
  converted to this* in /extended precision/. Numbers are converted from
  extended precision to singe- or double-precision format as they are
  stored in memory. *It means that storing a number from a register to  memory and then retrieving it back into the register can cause it to  change, due to rounding, underflow, or overflow.*
#+END_QUOTE

Floating-point arithmetic must be used very carefully, because it has
only limited range and precision, and because it does not obey common
mathematical properties such as associativity.
* Q&A in Machine-Level Representation of Programs
#+BEGIN_QUOTE
*Things You Should Know*

- **8086**: (1978, 29 K transistors). One of the first single-chip, 16-bit microprocessors. The 8088, a variant of the 8086 with an 8-bit external bus, formed the heart of the original IBM personal computers. The original models came with. 32768 bytes of memory and two floppy drives (no hard drive).  Architecturally, the machines were limited to a 655360-byte address space‚Äîaddresses were only 20 bits long (1048576 bytes addressable), and the operating system reserved 393216 bytes of its own use. In 1980, Intel introduced the 8087 floating-point coprocessor (45 K transistors) to operate alongside the 8086 or 8088 processor, executing the floating-point instructions. The 8087 established the floating-point model for the x86 line, often referred as "x87".

- **80286**: (1982, 134 K transistors). Added more (and now obsolete) addressing modes. Formed the basis of the IBM PC-AT personal computer, the original platform for MS Windows.

- **i386**: (1985, 275 K transistors). Expanded the architecture to 32 bits. Added the flat addressing model used by Linux and recent versions of the Windows family of operating system. This was the first machine in the series that could support a Unix operating system.

- **i486**: (1989, 1.2 M transistors). Improved performance and integrated the floating-point unit onto thee processor chip but did not significantly change the instruction set.

- **Pentium**: (1993, 3.1 M transistors). Improved performance, but only added minor extensions to the instruction set.

- **PentiumPro**: (1995, 5.5 M transistors). Introduced a radically new processor design, internally known as the *P6* microarchitecture. Added a call of "conditional move" instructions to the instruction set.

- **Pentium II**: (1997, 7 M transistors). Continuation of the P6 micro architecture.

- **Pentium III**: (1999, 8.2 M transistors). Introduced **SSE**, a calls of instructions for manipulating vectors of integer or floating-point data. Each datum can be 1, 2, or 4 bytes, packed into vectors of 128 bits. Later versions of this chip went up to 24 M transistors, due to the incorporation of the level-2cache on chip.

- **Pentium 4**: (2000, 42 M transistors). Extended SSE to SSE2, adding new data types (including double-precision floating-point), along with 144 new instructions for these formats. With these extensions, compilers can user SSR instructions, rather than x87 instructions, to compile floating-point code. Introduced the *NetBurst* microarchitecture, which could operate at very high clock speeds, but  at the cost of high power consumption.

- **Pentium 4E**: (2004, 125 M transistors). Addend *hyper-threading*, a method to run two programs simultaneously on a single processor, as well as EM64T, Intels implementation of a 64-bit extension to IAa32 developed by Advanced Micro Devices (AMD), which we refer to as x86-64.

- **Core 2**: (2006, 291 M transistors). Returned back to a microarchitecture similar to P6. First *multi-core* Intel microprocessor, where multiple processors are implemented on a single chip. Did not support hyper-threading.

- **Core i7**: (2008, 781 M transistors). Incorporated both hyper-threading and multi-core, with  initial version supporting two executing programs on each core and up to four cores on each chip.
#+END_QUOTE

** What does the option =-01= do in the following command line?
#+BEGIN_SRC bash
gcc -01 -o p p1.c p2.c # note the difference from the more general one
#+END_SRC

The command-line option =-01= instructs the compiler to apply **level one optimisations**. Increasing the level of optimisation makes the program run faster, but at a risk of increased compilation time and difficulties running debugging tools on the code. The higher optimisation level is, the harder to understand the relationships between the source code and the machine code.

** What is ISA, and what does it do?
ISA, short for "Instruction Set Architecture", defines the format and behaviour of the machine-level program, which include the processor state, the format of the instructions, and the effect each of these instructions will have on the state.

** Do you know which parts of the processor state are visible that normally hidden from the C programmer?
- The *program counter* (commonly referred to as the "PC", and called =%eip= in IA32) indicates the address in memory of the next instruction to be executed.

- The integer *register file*  contains eight named locations storing 32-bits values. These registers can hold integer data or addresses (corresponding to the C pointers). Some of the registers are used to keep track of critical parts of the program state, while others are used to hold temporary data, such as the local variables of the procedure, and the value to be returned by a function.

#+caption: IA32 integer registers
[[https://juicyiter.gitee.io/images/ia32-integer-registers.png]]

- The condition code registers hold status information about the most recently executed arithmetic or logical instruction. These are used to implement conditional changes in the control or data flow, such as is required to implement on the =if= and =while= statements.

- A set of floating-point registers are used to hold floating-point data.

#+caption: Sizes of C data type in IA32
[[https://juicyiter.gitee.io/images/sizes-of-c-data-types-in-ia32.png]]

#+BEGIN_QUOTE
IA32 doesn't provide hardware support for 64-bit integer arithmetic. Compiling =long long= data requires generating sequences of operations to perform the arithmetic in 32-bit chunks.
#+END_QUOTE

** What do you see in the following picture?

#+caption: IA32 integer registers
[[https://juicyiter.gitee.io/images/ia32-integer-registers.png]]

- The first six registers can be considered general-purpose registers, with no restriction placed.
- The 2 low-order bytes of the first four registers can be used or accessed independently.
- When a byte instruction updates one of these single-byte "register elements", the rest three bytes of the register remain unchanged.

** What about this one?

#+caption: Operand forms
https://juicyiter.gitee.io/images/operand-forms.png

- The first type, *immediate*, is for constant values, which is written with a =$= followed by an integer using the standard C notation. Any value that can fits into 32-bit word can be used, although the assembler will use 1- or 2-byte encodings, when possible.
- The second type, *register*, denotes the contents of one of the registers, either one of the eight 32-bit registers (e.g. =%eax=) for double-word operations, or one of the eight 16-bit registers (e.g. =%ax=) for single-word operations, or one the eight 8-bit registers (e.g. =%al=) for byte operations.
- The third type, *memory*, is for memory reference. The last form is the general form, which is often used when referencing elements of arrays. The other forms are simply special cases of the general form.

** Some important instructions!
#+caption: Data movement instructions
[[https://juicyiter.gitee.io/images/data-movement-instructions.png]]

#+caption: Illustration of stack operation
[[https://juicyiter.gitee.io/images/illustration-of-stack-operation.png]]

By convention, we draw stacks upside down, so that the "top" of the stack is shown at the bottom. IA32 stacks grow toward lower addresses, so pushing involves decrementing the stack pointer (register =%esp=) and storing to memory, while popping involves reading from memory and incrementing the stack pointer.

#+caption: Integer arithmetic operations
[[https://juicyiter.gitee.io/images/integer-arithmetic-operations.png]]

Only =leal= has no other size. The remaining ones are more standard unary or binary operations. Note the non-intuitive ordering of the operands with ATT-format assembly code.

Only right shift requires instructions that differentiate between signed data and unsigned data.

In general, compilers generate code that individual registers for multiple program values and moves program values among the registers.

#+caption: Special arithmetic operations
[[https://juicyiter.gitee.io/images/special-arithmetic-operations.png]]

These operations provide full 64-bit multiplication and division, for both signed and unsigned numbers. **For both of these, one argument must be in register =%eax=, and the other is given as the instruction source operand. The product is then stored in register =%edx= (high-order 32-bits) and `%eax` (low-order 32-bits). ** Although the name =imull= is used for two distinct multiplication operations, the assembler can tell which one is intended by counting the number of operands.

Some example code:

#+BEGIN_SRC asm
### Divid x by y
movl  8(%ebp), %edx   # Put x in %edx
movl  %edx, %eax      # Copy x to %eax
sarl  $31, %edx       # Sign extend x in %edx
idivl     12(%ebp)        # Divide by x
# Following the idivl instruction, the quotient and remainder
# are copied to the top two stack location.
movl  %eax, 4(%esp)   # Store x/y
movl  %edx, (%esp)    # Store x%y


### A more conventional method
movl  8(%ebp), %edx
cltd                  # Sign extend into %edx
idivl     12(%ebp)
movl  %eax, 4(%esp)
movl  %edx, (%esp)

### Unsigned division makes use of the divl instruction. Typically register %edx is set to 0 beforhand.
#+END_SRC

#+caption: Condition codes
[[https://juicyiter.gitee.io/images/condition-codes.png ]]

The CPU maintains a set of single-bit *condition code* registers describing attributes of the **most recent** arithmetic or logical operation. Above figure illustrates the most useful condition codes.

#+caption: Comparison and test instructions
[[https://juicyiter.gitee.io/images/comparison-and-test-instructions.png]]

The =CMP=  instructions behave in the same way as the =SUB= instructions, except that they set the condition codes without changing their destinations.

The =TEST= instructions behave in the same way as the =AND= instructions, except that they set the condition codes without changing their destinations.

#+caption: The SET instructions
[[https://juicyiter.gitee.io/images/the-set-instructions.png]]

The =SET= instructions set a **single byte** to 0 or 1 based on some combination of the condition codes. It's important to recognise that the suffix of these instructions denote **different conditions** and not different operand sizes.

#+caption: The JUMP instructions
[[https://juicyiter.gitee.io/imgaes/the-jump-instructions.png]]

In the *indirect* jump, the jump target is read from the a register or a memory location. When performing PC-relative addressing, the value of the program counter is the address of the instruction following the jump, not that of the jump itself. By using a PC-relative encoding of the jump targets, the instructions can be compactly encoded (requiring just 2 bytes), and the object code can be shifted to different positions in memory without alteration.

#+caption: The conditional move instructions
[[https://juicyiter.gitee.io/images/the-conditional-move-instructions.png]]

These instructions copy the source value *S* to its destination *R* when the move condition holds.

** Is there any restrictions when using data move instructions?
Basically, the source operand designates a **value** that is immediate, stored in a register, or stored in a memory, and the destination operand designates a **location** that is either a register or a memory. **But**, the source operand and the destination operand cannot both referred to a memory. Copying a value from one memory to another requires two instructions‚Äîthe first to load the value into a register, then write this register value to the destination.

** How can you copy a value from a small location to a larger location?

=movz= and =movs= will do the trick. As you can see from the data movement instructions figure above, in the last two suffixes, the first one always denotes smaller location.

** How can you derive the bytes an instruction requires from its 'pattern'?

The following code shows five possible combinations of source and destination types.

#+BEGIN_SRC asm
movl $0x4050, %eax			# Imediate -- Register,	4 bytes
movw %bp, %sp				# Register -- Register,	2 bytes
movb (%edi, %ecx), %ah		# Memory -- Register, 	1 byte
movb $-17, (%esp)			# Imediate -- Memory,	1 byte
movl %eax, -12(%ebp)		# Register -- Memory,	4 bytes
#+END_SRC

Other instructions which have two operands have the same juice.

=pop= and =push= instructions take a single operand‚Äîthe data source for pushing and the date destination for popping, which both require a single byte in machine code.

** Why bother =push= while you can use =sub= and =mov=?
Push a double-word value to the stack =push %ebp= requires a single byte.

#+BEGIN_SRC asm
subl $4, %esp
movl %ebp, %esp
#+END_SRC
But the pair of the instructions above require 6 bytes totally, so...

After =pop=, the "last top" value remains unchanged until it's overwritten again.

** What's wrong with the following code?

#+BEGIN_SRC asm
movb $0xF, (%bl)
movl %ax, (%esp)
movw (%eax), 4(%esp)
movb %ah, %sh
movl %eax, $0x123
movl %eax, %dx
movb %si, 8(%ebp)
#+END_SRC

1. Cannot use =%bl= as an address register
2. Mismatch of the data sizes
3. Cannot have both source and destination operand be memory referrence
4. No register named =%sh=
5. Cannot have the destination operand be immediate
6. Destination operand incorrect size
7. Mismatch of the data size of the source operand

#+caption: Sample code table
[[https://juicyiter.gitee.io/images/sample-code-table.png]]

Recall that when performing a cast that involves both a size change and a change of "signedness" in C, the operation should change the signedness first.

** What is the address of the =mov= instruction?

#+BEGIN_SRC asm
XXXXXXX:	74 12				je		8048391
XXXXXXX:	b8 00 00 00 00		mov		$0x0, %eax
#+END_SRC

Offsets: =0x12. 0x91 - 0x12 = 0x7f=. So the address of the =mov= instruction is 804837f and the address of the =je= instruction is =804837d= (2 bytes).

** What is the relationship between the annotation on the right and the byte coding on the left?

#+BEGIN_SRC asm
80482aa:	ff 25 fc 9f 04 08	jmp		*0x8049ffc
#+END_SRC

The first two bytes denote the instruction =jmp=, and this is an indirect jump, where the following 4 bytes are the jump location. And it's a little endian machine.

** What is the address of the jump target?
#+BEGIN_SRC asm
80482bf:	e9 e0 ff ff ff		jmp		XXXXXXX
80482c4:	90					nop
#+END_SRC

Note: In the code above, the jump target is encoded in PC-relative form as a 4-byte, two's-complement number. The bytes are listed from the least significant to most, reflecting the little-endian byte ordering of IA32.

Reading the bytes in reverse order, you can see that the target offset is =0xffffffe0=, or decimal -32. Adding this to =0x80482c4= gives the address =0x80482a4=.

** Why the assembly code contains two conditional branches, even though the C code has only one =if= statement?

#+BEGIN_SRC c
void cond(int a, int *p)
{
    if (p && a>0)
        *p += a;
}
#+END_SRC

Assembly code:
#+BEGIN_SRC asm
# a %ebp+8, p at %ebp+12
	movl	8(%ebp), %edx
	movl	12(%ebp), %eax
	testl	%eax, %eax
	je		.L3
	testl	%edx, %edx
	jle		.L3
	addl	%edx, (%eax)
.L3:
#+END_SRC

The first conditional branch is part of the implementation of the =&&= expression. If the test for =p= being non-null fails, the code will skip the test of =a>0=.

** How the compilers generate loop code?
Most compilers generate loop code based on the =do-while= form of a loop, even though this form is relatively uncommon in actual programs. Other forms of loops are transformed into =do-while= from and then compiled into machine code.

** Why can code based on *conditional data transfer* outperform code based on conditional control transfer?

Processors achieve high performance through *pipelining*, where an instruction is processed via a sequence of stages, each performing one small portion of the required operations (e.g. fetching the instructions from memory, determining the instruction type, reading from memory, and updating the program counter). This approach achieves high performance by **overlapping the steps of the successive instructions**, such as fetching one instruction while performing the arithmetic operations for a previous instruction. Processor employ sophisticated *branch prediction logic* to try to guess whether or not each jump instruction will be followed. Mispredicting, on the other hand, requires that the processor **discard** much of the work it has already done on future instructions and then begin filling the pipeline with instructions starting at the correct location.

** What's the differences between =jmp=, =set= and =cmov= ?
The outcomes of =cmov= instructions depends on the value of the conditional codes. The source value is read from either memory or the source register, but it is copied to the destination only if the specified condition holds.

Unlike the traditional jump moves, the processor can execute conditional move without having to predict the outcome of the test.

** What the peculiarities of  the conditional move are?

Single byte conditional moves are not supported. The assembly can infer the operand length of a conditional move instruction from the name of the destination register, and so the same instruction can be used for all operand lengths.

** How conditional operations can be implemented via conditional data transfers?

Consider the following code:
#+BEGIN_SRC c
int arith(int x)
{
    return x/4;
}
#+END_SRC

When compiled, GCC generates the following assembly code:

#+BEGIN_SRC asm
# register: x in %edx

leal 	3(%edx), %eax	# temp = x+3
testl 	%edx, %edx		# Test x
cmovns 	%edx, %eax		# If >= 0, temp = x
sarl 	$2, %eax		# Return value in %eax
#+END_SRC

The program creates a temporary value equal to =x+3=, in anticipation of =x= being negative, and therefore requiring **biasing**. The `cmovns` instructions conditionally changes the number to =x=, when =x>=0=, and then it is shifted by 2 to generate =x/4=;

The source code can be described by following code:

#+BEGIN_SRC c
int arith(int x)
{
    bias = x+3;
    if(x<0) x = bias;
    return x>>2;
}
#+END_SRC

** Heads up: JUMP TABLE
A jump table is an array where entry =i= is the address of a code segment implementing the action the program should take when the switch index equals =i=.

#+BEGIN_SRC asm
jmp		*.L7(, %eax, 4)
#+END_SRC

In the assembly code above, the =jmp= instruction's operand is prefixed with =*=, indicating **indirect jump**, and the operand specifies a memory location indexed by register =%eax=, which holds the value of =index=.

The jump table:
#+BEGIN_SRC asm
.section		.rodata		# Read-only data
	.align 4					# Align address to multiple of 4
.L7:
	.long	.L3					# Case one: .L3
	.long 	.L2
	.long 	.L4
	.long	.L5
#+END_SRC

Label =.L7= marks the start of this allocation. The address associated with this label serves as the base for the indirect jump.

** Consider the following code and answer what the values are of the case labels in the `switch` statement body.

#+BEGIN_SRC c
int switch2(int x)
{
    int result = 0;
    switch(x)
    {
            /* body of switch statement omitted */
    }
}
#+END_SRC

#+BEGIN_SRC asm
# x at %ebp+8
	movl	8(%ebp), %eax
	# Set up jump table access
	addl	$2, %eax
	cmpl	$6, %eax
	ja		.L2
	jmp		*.L8(, %eax, 4)
	...

	# Jump table for switch2
	.L8:
		.long	.L3
		.long	.L2
		.long	.L4
		.long	.L5
		.long 	.L6
		.long 	.L6
		.long	.L7
#+END_SRC

Compare =x+2= and =6=, so the possible values are: =-2, -1, 0, 1, 2, 3, 4=. But the =.L2= is used to return, so =0= is missing. Hence, the values are: =-2, -1, 1, 2, 3, 4=. Besides, case =.L6= has two labels in C.

** Procedures
The most important features **in IA32** when handling procedure calls is ***stack frame structure***.
*** So what is a stack frame structure?
A stack frame is the portion of the stack allocated for **a single** procedure call, which means every procedure call has a stack frame pointer. See more at the following picture.

#+caption: Stack frame structure
[[https://juicyiter.gitee.io/images/stack-frame-structure.png]]

The register =%ebp= serves as the frame pointer, and the register =%esp= serves as the stack pointer. The stack pointer moves (**in IA32**) while the procedures is executing and **most information is accessed relative to the stack pointer**. Also, note that **the *return address* within caller** where the program should resume execution when it returns from callee **is pushed onto the stack**.

*** Transferring control

#+caption: Illustration of =call= and =ret= functions
[[https://juicyiter.gitee.io/images/illustration-of-call-and-ret-functions.png]]

The above figure is to illustrate the execution of the =call= and =ret= instructions for the following functions:

#+BEGIN_SRC asm
# Beginning of function sum
08048394  <sum>:
 8048394:	55					push	%ebp
...
# Return from function sum

 80483a4:	c3					ret
...

# Call to sum from main
 80483dc:	e8 b3 ff ff ff		call	8048394 <sum>
 80483e1:   83 c4 14			add		$0x14, %esp
#+END_SRC

The effect of a =call= instruction is to **push a return address(=0x080483e1=) on the stack** and **jump to the start of the called procedure**(=0x08048394=). The return address is the address of the instruction **immediately following the =call= in the program**, so that execution will resume at this location when the called procedure returns. The =ret= instruction **pops an address off the stack(`0x080483e1`) and jumps to this location.**

n conclusion, the =call= instruction transfers control to the start of a function, while the  =ret= instruction returns back to the instruction following the call.

#+BEGIN_QUOTE
Register =%eax= (in IA32) is used for returning the value from any function that returns an **integer or pointer**.
#+END_QUOTE

*** Some register usage conventions
**Caller-save** registers: =%eax=, =%edx= and =%ecx=. When a caller procedure P calls a callee procedure Q, Q can overwrite these registers without destroying any date required by P.

**Callee-save** registers: =%ebx=, =%esi= and =%edi=. Q must **save** the values of these registers on the stack before overwriting them, and **restore** them before returning, because the P may need these values for its future use(like recursive procedures).

*** Example codes
#+BEGIN_SRC c
int swap_add(int *xp, int *yp)
{
    int x = *xp;
    int y = *yp;

    *xp = y;
    *yp = x;
    return x + y;
}
int caller()
{
    int arg1 = 534;
    int arg2 = 1057;
    int sum = swap_add(&arg1, &arg2);
    int diff = arg1 - arg2;

    return sum * dif;
}
#+END_SRC

The following assembly code shows the compiled version of =caller= and =swap_add=:
#+BEGIN_SRC asm
caller:
  pushl		%ebp				# Save old %ebp
  movl		%esp, %ebp			# Set %ebp as frame pointer
  subl 		$24, %esp			# Allocate 24 bytes on stack
  movl		$534, -4(%ebp)		# Set arg1 to 534
  movl		$1057, -8(%ebp)		# Set arg2 to 1057
  leal		-8(%ebp), %eax		# Compute &arg2
  movl		%eax, 4(%esp)		# Store on stack
  leal		-4(%ebp), %eax		# Compute &arg1
  movl		%eax, (%esp)		# Store on stack
  call		swap_add			# Call the swap_add function
  movl		-4(%ebp), %edx		# Get arg1
  subl		-8(%ebp), %edx		# Compute arg1 - arg2
  imull		%edx, %eax			# Compute sum * diff
  leave							# Deallocate a stack frame
  ret

swap_add:
  # Setup code
  pushl		%ebp				# Save old %ebp
  movl		%esp, %ebp			# Set %ebp as frame pointer
  pushl		%ebx				# Save %ebx
  # Body code
  movl 		8(%ebp), %edx		# Get xp
  movl		12(%ebp), %ecx		# Get yp
  movl		(%edx), %ebx		# Get x
  movl		(%ecx), %eax		# Get y
  movl 		%eax, (%edx)		# Store y at xp
  movl		%ebx, (%ecx)		# Store x at yp
  addl		%ebx, %eax			# Return value = x+y
  # Finishing code
  popl		%ebx				# Restore %ebx
  popl		%ebp				# Restore %ebp
  ret							# Return
#+END_SRC

#+caption: Stack frame for =caller= and =swap_add=
[[https://juicyiter.gitee.io/images/stack-frame-for-caller-and-swap-add.png]]

*** Why does GCC allocate space that never gets used?
We see that the code generated by GCC for =caller= allocates 24 bytes on the stack even though it only makes use of 16 of them. Why is that?

GCC adheres to an x86 programming guideline that the **total stack space used by the function** should be a multiple of 16 bytes. Including 4 bytes for the saved value of =%ebp= and the 4 bytes for the return address, =caller= uses a total of 32 bytes.

*** Recursive procedures
#+BEGIN_SRC c
int rfact(int n)
{
    int result;
    if(n <= 1)
        return 1;
    else
        result = n * rfact(n-1);
    return result;
}
#+END_SRC

#+BEGIN_SRC asm
# Argument: n at %ebp+8
# Registers: n in %ebx, result in %eax
rfact:
  pushl		%ebp				# Save old %ebp
  movl 		%esp, %ebp
  pushl		%ebx				# Save callee save register %ebx
  subl		$4, %esp
  movl		8(%ebp), %ebx
  movl		$1, %eax
  cmpl		$1, %ebx
  jle		.L53
  leal		-1(%ebx), %eax
  movl		%eax, (%esp)
  call		rfact
  imull		%ebx, %eax
#+END_SRC

** Array allocation and access
GCC allocates 12 bytes(16 bytes in x86_64) for the =long double= format, even though it only requires 10 bytes.

The =leal= instruction is used to generate an address, while =movl= is used to reference memory.

** Heterogeneous data structures
Consider the following declarations:

#+BEGIN_SRC c
struct S3{
    char c;
    int i[2];
    double v;
};
union U3{
    char c;
    int i[2];
    double v;
};
#+END_SRC

When complied on an IA32 Linux machine, the offsets of the fields, as well as the total size of data types =S3= and =U3=, are as shown in the following figure:

#+caption: Struct and union alignment
[[https://juicyiter.gitee.io/images/struct-and-union-alignment.png]]

*** Why =i= has offset 4 in =S3= rather than 1?

For code involving structures, the compiler may need to **insert gaps** in the field allocation to ensure that each structure element satisfies its alignment requirement.

#+BEGIN_QUOTE
Linux follows an alignment policy where 2-byte data types (e.g. =short=) must have an address that is a multiple of 2, while any larger data types (e.g. =int=, =int *=, =float=, and =double=) must have an address that is a multiple of 4.  This requirement means that the least significant bit or low-order 2 bits of the address of the object must equal zero.

Most computer systems requires that the address for some type of object must be a multiple of some value K (typically 2, 4, or 8). Such alignment restrictions simplify the design of the hardware forming the interface between the processor and the memory system.
#+END_QUOTE

** What happend after buffer overflow?
Consider the following C code:
#+BEGIN_SRC c
void echo()
{
    char buf[8];	/* Way too small */
    gets(buf);
    puts(buf);
}
#+END_SRC

And the assembly code generated by GCC:

#+BEGIN_SRC asm
echo:
  pushl		%ebp			# Save %ebp
  movl		%esp, %ebp		# Frame pointer
  pushl		%ebx			# Save %ebx
  subl		$20, %esp		# Allocate 20 bytes
  leal		-12(%ebp), %ebx	# Compute buf as %ebp - 12
  movl		%ebx, (%esp)	# Store buf at top of stack
  call		gets			# Call gets
  movl		%ebx, (%esp)	# Store buf at top of stack
  call		puts			# Call puts
  addl		$20, %esp		# Deallocate
  popl		%edx			# Restore
  popl		%ebp			# Restore
  ret
#+END_SRC
And the stack organization for =echo= function.

#+caption: Stack organization for =echo= function
[[https://juicyiter.gitee.io/images/stack-organization.png]]

A long string will cause the =gets= to overwrite some of the information stored on the stack.

The corruption is cumulative‚Äîas the number of the characters increases, more state gets corrupted.

** Three ways to thwart buffer overflow

*** Stack randomization
This is implemented by allocating a random amount of space between 0 and n bytes on the stack at the start of a program, for example, by using the allocation function =alloca=, which allocates space for a specified number of bytes on the stack.

It's one of a larger class of techniques known as *address-space layout randomization*, or ALSR. With ALSR, different parts of the program, including program code, library code, stack, global variables, and heap data, are loaded into different regions of memory each time a program is run.
*** Stack corruption detection
The idea is to store a special **canary value** in the stack frame between any local buffer and the rest of the stack state, as illustrated in the following figure.

#+caption: Stack organization for =echo= function with stack protector enabled
[[https://juicyiter.gitee.io/images/stack-corruption-detection.png]]

The canary value, also referred to as a *guard value*, is generated randomly each time the program is run, so there is no easy way for an attacker to determine what it is.

The modified assembly code is as follow:
#+BEGIN_SRC asm
...
  subl 	$20, %esp
  movl 	%gs:20, %eax			# Retrieve canary
  movl 	%eax, -8(%ebp)			# Store on stack
  xorl 	%eax, %eax				# Zero out register
  leal 	-16(%ebp), %ebx
  ...
  movl 	-8(%ebp), %eax			# Retrieve canary
  xorl 	%gs:20, %eax			# Compare to stored value
  je   	.L19
  call 	__stack_chk_fail		# Stack corrupted!
 .L19:
  addl	$20, %esp				# Normal return
  ...
#+END_SRC
The instruction argument =%gs:20= is an instruction that the canary value is read from memory using ***Segment addressing***, an addressing mechanism that dates back to the 80286 and is seldom found in programs running on modern systems. By storing the canary value in a special segment, it can be marked as **read only**.

*** Limiting executable code regions
This method is to limit which regions hold executable code. Historically, the x86 architecture merged the read and execute access controls into a single 1-bit flag, so that any page marked as readable was also executable. The stack had to be kept both readable and writable, and therefore the bytes on the stack were also executable.

AMD introduced an "NX" (for "no-executa") bit into the memory protextion for its 64-bit processors, seperating the read and execute acess modes, and Intel followed suit. With this feature, the stack can be marked as being readable and writable, but not executable, and the checking of whether a page is executable is performed in hardware, with no penalty in efficiency.

** Some of the key differences between IA32 and x86-64
- =movq= in x86-64, =movl= in IA32. The **pointers** and variables declared as =long= integers are 64 bits (**quad words**) in x86-64 rather than 32 bits (long words) in IA32.

- The number of registers has been double to 16 64-bit versions of registers. The procedure returns a value by storing it in register =%rax=. The lower-order 32 bits, 16 bits and 8 bits of each register can be accessed directly in x86-64.

  #+caption: 64-bit integer registers
  [[https://juicyiter.gitee.io/images/64-bit-integer-registers.png]]

- No stack frame gets generated in the x86-64 version. =%ebp= is available for use as a general-purpose register.

- Arguments are passed in **6 argument registers** respectively.

- With x86-64, the program counter is named for =%rip=.

- New instruction =movabsq=.

  #+caption: 64-bit data movement instructions
  [[https://juicyiter.gitee.io/images/64-bit-data-movement-instructions.png]]

  #+BEGIN_QUOTE
  The =movabsq= instruction **only allows immediate data (show as *I*) as the source value**.
  #+END_QUOTE

- =movl= and =addl= instructions in x86-64 will set upper 32 bits of the destination register to zero. Instructions that generate 16-bit results , such as =addw=, only affect ther 16-bit destination registers, and similarly for instructions that generate 8-bit results. As with the =movq= instruction, **immediate operands are limited to 32-values**, which are sign extend to 64 bits.

  #+caption: 64-bit special arithmetic operations
  [[http://juicyiter.gitee.io/images/64-bit-special-arithmetic-operations.png]]

  #+BEGIN_QUOTE
  =cltq= is equivalent to =movslq=.
  #+END_QUOTE

- Special instruction =rep= in x86-64. It is normally used to implement a repeating string operation to avoid making the =ret= instruction be the destination of a conditional jump instruction. It serves as a form of no-operation here to make the code faster on AMD processors.

- Basicly, arguments are passed to procedures via registers rather than stack. But there are few cases when the stack frame is needed:

  - Too many local variables.
  - Some local variables are arrays or structures.
  - The function uses the address-of operator (&) to compute the address of a local variable.
  - The function must pass some arguments on the stack to another function.
  - The function needs to save the state of a callee-save register before modifying it.

  The stack frames for x86-64 procedures usually have a fixed size, set at the beginning of the procedure by decrementing the stack pointer (=%rsp=). **The stack pointer remains at a fixed size during the call**, making it possible to acess data using offsets relative to the stack pointer, so the frame pointer is no longer needed.

- The =callq= instruction stores a 64-bit return address on the stack.

- Being able to access memory **beyond the stack pointer** in x86-64. It's an unusual feature. The x86-64 ABI specifies that programs can use **128 bytes beyond** the current stack pointer. This area is referred to as the *red zone*. It must be kept available for reading and writing as the stack pointer moves.
