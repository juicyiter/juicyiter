#+TITLE:    TensorFlow
#+AUTHOR:   Ethan Mengoreo
#+EMAIL:    mengoreo@163.com
#+STARTUP:  indent
#+LANGUAGE: en
#+OPTIONS:  toc:t num:0
#+SETUPFILE: https://juicyiter.gitee.io/assets/mengoreo.setup

* 声明式编程

Consider Fibonacci
#+caption: 命令式编程
#+BEGIN_SRC python
def fib(n):
    a, b = 1, 1
    for i in range(1, n):
        a, b = b, a + b
    return a
#+END_SRC

#+caption: 声明式编程
#+BEGIN_SRC python
fib = lambda x: 1 if x <= 2 else fib(x - 1) + fib(x - 2)
return fib(3)
#+END_SRC


* Keras
First thing's first.
#+BEGIN_SRC python :session
  import tensorflow as tf
  from tensorflow.keras import layers
#+END_SRC

#+RESULTS:

Test versions
#+BEGIN_SRC python :results output :session :exports both
  print(tf.VERSION)
  print(tf.keras.__version__)
#+END_SRC

#+RESULTS:
: 1.13.1
: 2.2.4-tf

** Build a simple model
The most common type of model is stack of layers: =tf.kerals.Sequential= model.

#+BEGIN_SRC python
  model = tf.keras.Sequential()

  # Adds a densely-connected layer with 64 units to the model
  model.add(layers.Dense(64, activation='relu'))

  model.add(layers.Dense(64, activation='relu'))

  model.add(layers.Dense(10, activation='softmax'))
#+END_SRC
*** Configure the layers

- =activation=: Set the activation function for the layer. *By default, no activation is applied*

- =kernel_initializer= and =bias_initializer=: The initialization schemes that create the layer's weights (kernel and bias). *This defaults to the =Glorot uniform= initializer*.

- =kernel_regularizer= and =bias_regularizer=: The regularisation schemes that create the layer's weights (kernel and bias). *By default, no regularisation is applied*.

#+BEGIN_SRC python
  # sigmoid layer
  layers.Dense(64, activation='sigmoid')
  # or:
  layers.Dense(64, activation=tf.sigmoid)

  # A linear layer with L2 regularisation of factor  0.01 applied to the kernel matrix:
  layer.Dense(64, kernel_initializer=tf.keras.regularizer.l1(0.01))

  # L2
  layer.Dense(64, kernel_initializer=tf.keras.regularizer.l2(0.01))

  # A linear layer with a kernel initialized to a random orthogonal matrix:
  layer.Dense(64, kernel_initializer='orthogonal')

  # A linear layer with a bias vector initialized to 2.0s:
  layer.Dense(64, bias_initializer=tf.keras.initializers.constant(2.0))

#+END_SRC

** Train and evaluate

#+BEGIN_SRC python :session
  model = tf.keras.Sequential([
      tf.keras.layers.Dense(64, activation='relu', input_shape=(32, )),
      tf.keras.layers.Dense(64, activation='relu'),
      tf.keras.layers.Dense(10, activation='softmax')
  ])

  model.compile(optimizer=tf.train.AdamOptimizer(0.001),
                loss='sparse_categorical_crossentropy',
                metrics=['accuracy']
  )


#+END_SRC

#+RESULTS:

- =optimizer=: This object specifies the training procedure. Pass it optimizer *instances* from the =tf.train= module, such as =tf.train.AdamOptimizer=, =tf.train.RMSPropOptimizer= or =tf.train.GradientDescentOptmizer=.

- =loss=: The function to minimize during optimization. Common choices include *mean squared error* (=mse=), =categorical_crossentropy=, and =binary_crossentropy=. Loss functions are specified by name or by passing a callable object from the =tf.keras.losses=.

- =metrics=: Used to monitor training. These are string names of callables from the =tf.keras.metrics= module.

#+BEGIN_SRC python
  model.compile(optimizer=tf.train.AdamOptimizer(0.01),
                loss='mse',
                metrics=['mae']) # Mean absolute error

  model.compile(optimizer=tf.train.RMSPropOptimizer,
                loss=tf.keras.losses.categorical_crossetropy,
                metrics=[tf.keras.metrics.categorical_accuracy])


#+END_SRC
