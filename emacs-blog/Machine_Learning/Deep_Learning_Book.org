#+TITLE:    Notes of Deep Learning Book
#+AUTHOR:   Ethan Mengoreo
#+EMAIL:    mengoreo@163.com
#+STARTUP:  indent
#+LANGUAGE: en
#+OPTIONS:  toc:t num:3
#+SETUPFILE: https://juicyiter.gitee.io/assets/mengoreo.setup

* Linear Algebra

** Basic Types

-  /Scalars/: A scalar is just a single number, usually in lower-case
   variable names.

-  /Vectors/: An array of numbers, with lower case names written in bold
   typeface. $$
     \boldsymbol{x} = \begin{bmatrix} x_1 \\ x_2 \\ \cdots \\ x_n\end{bmatrix} .
     $$

-  /Matrices/: A 2-D array of numbers, usually given in upper-case
   variable names with bold typeface, Such as $\boldsymbol{M}$.

-  /Tensors/: An array with more than two axes, denoted by this
   typeface: $\boldsymbol{\mathsf{A}}$ (in latex:
   \boldsymbol{\mathsf{A}}).

** Linear Dependence and Span

If a vector $\boldsymbol{m}$ is linear dependent with a vector
$\boldsymbol{n}$, then $\boldsymbol{m}​$ can be represented by
$\boldsymbol{n}$ with $\lambda$ multiplied with it, viz.
$\boldsymbol{m} = \lambda \boldsymbol{n}$, where
$\lambda\in \mathbb{R}, \lambda \neq 0​$.

The /span/ of a set of vectors is the set of all points obtainable by
linear combination of the original vectors. For example, the span of
$\boldsymbol{v_1}$,$\boldsymbol{v_2}$ and $\boldsymbol{v_3}$ is
$\{ \boldsymbol{v_1} + \boldsymbol{v_2} + \boldsymbol{v_3}, \boldsymbol{v_1} + 2\boldsymbol{v_2} + 3\boldsymbol{v_3}, \dots, m \boldsymbol{v_1}+ n\boldsymbol{v_2} + k\boldsymbol{v_3} \}$,
where $m$, $n$ and $k$ all can be any real numbers but zero.

** Celebrities' Norm

*** Euclidean norm

When the $p$ is equal to $2$, the /Euclidean norm,/ $L^2$ came in. It is
simply the euclidean distance from the origin to the point identified by
$x$. The Euclidean norm is used so frequently in machine learning that
it is often denoted as $||x||$ with subscript $2$ omitted. The /squared/
$L^2$ can also be used to calculate the size of a vector, simply as
$x^{\top}x$.

*** $L^1$ norm

In several machine learning applications, it is important discriminate
between elements that are exactly zero and elements that are very small
and close to zero. In these cases, we turn to a function that grows at
the same rate in all locations, but retains mathematical simplicity:
/the $L^1$ norm/.

$$||x||_1 = \displaystyle \sum_i|x_i| .$$

The $L^1$ is commonly used in machine learning when the difference
between zero and nonzero elements is very important.

*** Max norm

Another norm that commonly arises in machine learning applications is
the $L^{\infty}$, A.K.A the /max norm/. This norm implies to the
absolute value of the elements with largest magnitude in the vector:

$$||x||_\infty = \displaystyle \text{max}_i|x_i|.$$

<<Frobenius>> *Frobenius norm*

Sometimes, we may also want to evaluate the size of a matrix. In the
context of deep learning, the most common way is using the obscure
/Frobenius norm/:

$$||A||_F = \displaystyle \sqrt {\sum_{i, j} A_{i, j}^2},$$

which is analogous to the Euclidean norm.

** Orthogonal Matrix

An /orthogonal matrix/ is a square matrix whose rows are mutually
orthonormal and whose columns are mutually /orthonormal/: $$
\boldsymbol{A^{\top} A} = \boldsymbol{A A^{\top}} = \boldsymbol{I} .
$$

This implies that $\boldsymbol{A^{-1}} = \boldsymbol{A^{\top}}$,so
orthogonal matrices are of interest because their inverse is very cheap
to compute.

#+BEGIN_QUOTE
Pay careful attention to the definition of the orthogonal matrices.
Counterintuitively, their rows are note merely orthogonal but /fully
orthonormal/.
#+END_QUOTE

<<eigen>>
** Eigendecomposition

In this loooong eigendecomposition, we decompose a matrix into a set of
/eigenvectors/ and /eigenvalues/.

An eigenvectors of a square matrix $\boldsymbol{A}$ is a non-zero vector
$\boldsymbol{v}$ such that multiplication by $\boldsymbol{A}$ alters
only the scale of $\boldsymbol{v}$:
$\boldsymbol{Av} = \lambda \boldsymbol{v}$. The scalar $\lambda$ is
known as eigenvalue corresponding to this eigenvector.

If $\boldsymbol{v}$ is an eigenvector of $\boldsymbol{A}$, then so is
any rescaled vector $s\boldsymbol{v}$ for $s \in \mathbb{R}, s \neq 0$.
Moreover, $s \boldsymbol{v}$ still has the same eigenvalue. For this
reason, we only look for /unit eigenvectors/.

Suppose a matrix $\boldsymbol{A}$ has $n$ linear independent
eigenvectors, and we use these eigenvectors to form a matrix
$\boldsymbol{V}$ with one eigenvector per column. Likewise, we
concatenate the eigenvalues to form a vector $\boldsymbol{\lambda}$. The
/eigendecomposition/ of $\boldsymbol{A}$ is then given by $$
\boldsymbol{A} = \boldsymbol{V} \text{diag}(\boldsymbol{\lambda}) \boldsymbol{V^{-1}} .
$$ Specifically, every /real symmetric/ matrix can be decomposed into an
expression using only real-valued eigenvectors and eigenvalues:

$$
\boldsymbol{A} = \boldsymbol{Q} \boldsymbol{\Lambda} \boldsymbol{Q^{-1}} .
$$

where $\boldsymbol{Q}$ is an orthogonal matrix composed of eigenvectors
of $\boldsymbol{A}$, and $\boldsymbol{\Lambda}$ is a diagonal matrix.
The eigenvalue $\Lambda_{i, j}$ is associated with the eigenvectors in
column $i$ of $\boldsymbol{Q}$, denoted as $\boldsymbol{Q}_{:, i}$.
Because $\boldsymbol{Q}$ is an orthogonal matrix, we can think of
$\boldsymbol{A}$ as scaling space by $\lambda_i$ in direction
$\boldsymbol{v^{(i)}}$ .

#+CAPTION: Eigendecomposition
[[https://juicyiter.gitee.io/images/image-20190305161308681.png]]

The eigendecomposition of a matrix tells us many useful facts about the
matrix.

-  The matrix is /singular/ if and only if /any of the eigenvalues are
   zero/.
-  The eigendecomposition of a real symmetric matrix can also be used to
   optimise quadratic expression of the form
   $f(\boldsymbol{x}) = \boldsymbol{x^{\top}Ax}$ subject to
   $||\boldsymbol{x}||_2 = 1$. Whenever $\boldsymbol{x}$ is equal to an
   eigenvector of $\boldsymbol{A}$, $f$ takes on the value of the
   corresponding eigenvalue. The maximum value of $f$ within the
   constraint region is the maximum value of eigenvalue and its minimum
   value within the constraint region is the minimum eigenvalue.

A matrix whose eigenvalues are all positive is called /positive
definite/. A matrix whose eigenvalues are all /non-negative/ is called
positive *semidefinite*. Likewise, so are the negative definite and
negative semidefinite.

#+BEGIN_QUOTE
Positive semidefinite matrices are interesting because /they guarantee
that $\forall \boldsymbol{x}, \boldsymbol{x^{\top}Ax} \ge 0$/.

Positive definite matrices additionally guarantee that
$\boldsymbol{x^{\top}Ax} = 0 \implies \boldsymbol{x} = \boldsymbol{0}$
#+END_QUOTE

** Singular Value Decomposition (SVD)

/Every matrix has a singular value decomposition/, but the same is not
true for the eigenvalue decomposition. The singular decomposition is
similar to [[eigen][eigendecomposition]], except this time we will write
$\boldsymbol{A}$ as a product of three matrices:

$$ \boldsymbol{A} = \boldsymbol{UDV^{\top}}. $$

Suppose that $\boldsymbol{A}$ is an $m \times n$ matrix. Then
$\boldsymbol{U}$ is defined to be an $m \times m$ matrix,
$\boldsymbol{D}$ to be an $m \times n$ matrix, and $\boldsymbol{V}$ to
be an $n \times n$ matrix. The matrix $\boldsymbol{U}$ and
$\boldsymbol{V}$ are both defined to be /orthogonal/ matrices. The
matrix $\boldsymbol{D}$ is defined to /diagonal/ matrix.

#+BEGIN_QUOTE
Note that $\boldsymbol{D}$ is /not necessary square/.
#+END_QUOTE

The elements along the diagonal of $\boldsymbol{D}$ are /singular
values/. The columns of $\boldsymbol{U}$ and $\boldsymbol{V}$ are
/left-singular vectors/ and /right-singular vectors/ respectively.

#+BEGIN_QUOTE
The left-singular vectors of $\boldsymbol{A}$ are the eigenvectors of
$\boldsymbol{AA^{\top}}$. And the right-singular vectors of
$\boldsymbol{A}$ are the eigenvectors of $\boldsymbol{A^{\top}A}$.
#+END_QUOTE


[[https://en.wikipedia.org/wiki/Moore--Penrose\_inverse#Examples][The Moore-Penrose Pseudoinverse]]

The pseudoinverse of $\boldsymbol{A}$ is defined as a matrix $$
\boldsymbol{A^{+}} = \displaystyle \lim_{\alpha \to 0}(\boldsymbol{A^{\top}A} + \alpha\boldsymbol{I})^{-1}\boldsymbol{A^{\top}} .
$$ Practical algorithms for computing the pseudoinverse are not based on
this definition, but rather the formula $$
\boldsymbol{A^{+}} = \boldsymbol{VD^+U^{\top}} .
$$ where $\boldsymbol{V}$, $\boldsymbol{D}$ and $\boldsymbol{U}$ are the
singular value decomposition of $\boldsymbol{A}$, and the pseudoinverse
$\boldsymbol{D^+}$ of a diagonal matrix $\boldsymbol{D}$ is /obtained by
taking the resiprocal of its non-zero elements then taking the transpose
of the resulting matrix./

When $\boldsymbol{A}$ has more columns than its rows, it provides the
solution $\boldsymbol{x} = \boldsymbol{A^+y}$ with /minimal Euclidean
norm/ $||\boldsymbol{x}||_2$ /among all possible solutions/.

When $\boldsymbol{A}$ has more rows than its columns, it provides the
$\boldsymbol{x}$ which $\boldsymbol{Ax}$ is as close as possible to
$\boldsymbol{y}$ in terms of Euclidean norm
$||\boldsymbol{Ax}-\boldsymbol{y}||_2$

** The Trace Operator

$$
\text{Tr}(\boldsymbol{A}) = \displaystyle \sum_i \boldsymbol{A}_{i,i} .
$$

Then we can re-write the [[Frobenius][Frobenius norm]] of a matrix

$$||A||_F = \sqrt{\text{Tr}(\boldsymbol{AA^{\top}})} .$$

The trace operator is invariant to transpose operator and cyclic
permutations.

$$
\text{Tr}(\boldsymbol{A}) = \text{Tr}(\boldsymbol{A^{\top}}) .
$$

$$
\text{Tr}(\boldsymbol{ABC}) = \text{Tr}(\boldsymbol{BCA})=\text{Tr}(\boldsymbol{CAB}) .
$$

$$
\text{Tr}(\displaystyle \prod_{i=1}^{n} \boldsymbol{F^{(i)}}) = \text{Tr}(\boldsymbol{F^{(n)}}\displaystyle \prod_{i=1}^{n-1} \boldsymbol{F^{(i)}}) .
$$

Even if the resulting product has a different shape. For instance, for
$\boldsymbol{A} \in \mathbb{R}^{m \times n}$ and
$\boldsymbol{B} \in \mathbb{R}^{n \times m}$ $$
\text{Tr}(\boldsymbol{AB}) = \text{Tr}(\boldsymbol{BA}) .
$$

** The Determinant

The determinant of a /square matrix/, denoted as
$\text{det}(\boldsymbol{A})$ is a function that maps the matrices to
real scalars. The determinant is equal to the product of all the
eigenvalues of the matrix. The absolute value of it can be thought of as
a measure of **how much multiplications by the matrix expands or
contracts space.

* Probability

Three possible source of uncertainty:

-  /Inherent stochasticity/ in the system being modeled. For example, we
   can crate a theoretical scenarios that we postulate to have random
   dynamics, such as a hypothetical card game, where we assume the cards
   are truly shuffled into a random order.
-  /Incomplete observations/. Even deterministic systems can appear
   stochastic when we cannot observe all variables that drive the
   behaviour of the system.
-  /Incomlete modeling/. When we use a model that must discard some of
   the information we have observed, the discarded information might
   result in uncertainty in the model's predictions.

In many cases, it is more practical to use a /simple but uncertain
rule/, rather than a complex but certain one.

** Frequentist probability V.S. Bayesian probability

The /frequentist/ sees the probability as the /long-run/ expected
frequency of occurrence. $P(A) = n/N$, where $n$ is the number of times
even $A$ occurs in the $N$ opportunities.

The Bayesian view of probability is related to /degree of belief/. It is
a measure of the /plausibility/ of an event given /incomplete/
knowledge.

** Basic Concepts

-  Expectation $$
     \mathbb{E}_{\mathtt{x} \sim P}[f(x)] = \displaystyle \sum_x P(x) f(x) .
     $$

$$
   \mathbb{E}_{\mathtt{x} \sim p}[f(x)] = \int p(x)f(x) .
   $$

-  Variance $$
     \text{Var}(f(x)) = \mathbb{E}\left[(f(x) - \mathbb{E}[f(x)])^2 \right] .
     $$

-  Standard deviation: the square root of the variance.

-  Covariance $$
     \text{Cov}(f(x), g(y)) = \mathbb{E}[(f(x) - \mathbb{E}[f(x)])(g(y)-\mathbb{E}[g(y)])] .
     $$ The covariance matrix $$
     \text{Cov}(\mathbf{x})_{i, j} = \text{Cov}(\mathtt{x}_i, \mathtt{x}_j) .
     $$

** Common Probability Distribution

*** Bernoulli Distribution

The Bernoulli distribution is a distribution over a /binary/ random
variable. It has the following properties:

-  $P(\mathrm{x}=1) = \phi$
-  $P(\mathrm{x}=0) = 1- \phi$
-  $P(\mathrm{x}=x) = \phi^x(1-\phi)^{1-x}$
-  $\mathbb{E}_{\mathrm{x}}[\mathrm{x}] = \phi$
-  $\text{Var}_x(\text{x}) = \phi(1 - \phi)$

*** Multinoulli Distribution

The multinoulli distribution or /categorical/ distribution is a
distribution over /a single discrete variable with $k$ different states,
where $k$ is finite/. The $x$ th state with probability :
$C_n^x p^x(1-p)^{k-x}$, where $p \in [0, 1]$.

*** Gaussian Distribution

The most commonly used distribution over real numbers is the /normal
distribution/, also known as /Gaussian distribution/: $$
\mathcal{N}(x;\mu, \sigma^2) = \sqrt{ \frac{1}{2\pi\sigma^2} } \text{exp} \left( - \frac{1}{2\sigma^2}(x-\mu)^2\right) .
$$

The two parameters $\mu \in \mathbb{R}$ and $\sigma \in (0, \infty)$
control the normal distribution. The parameter $\mu$ gives the
coordinate of the central peak. This is also the /mean/ of the
distribution: $\mathbb{E}[\text{x}] = \mu$. The /standard deviation/ of
the distribution is given by the $\sigma$, and the /variance/ by
$\sigma^2$.

#+CAPTION: Normal Distribution
[[https://juicyiter.gitee.io/images/IMG_5F8D12450695-1.jpeg]]

Many distributions we wish to model are truly close to being normal
distributions. The /central limit theorem/ shows that the sum of many
independent random variables is /approximately/ normally distributed.

The normal distribution encodes the maximum amount of the uncertainty
over the real numbers.

The normal distribution generalises to $\mathbb{R}^n$, in which case it
is known as the /multivariate normal distribution/. It may parametrised
with a /positive definite/ (with all positive $\lambda$, A.K.A,
eigenvalues) symmetric matrix $\boldsymbol{\Sigma}$: $$
\mathcal{N}(\boldsymbol{x}; \boldsymbol{\mu}, \boldsymbol{\Sigma}) = \sqrt{\frac{1}{(2\pi)^n \text{det}(\boldsymbol{\Sigma)}}} \text{exp} \left( - \frac{1}{2}(\boldsymbol{x} - \boldsymbol{\mu})^\top \boldsymbol{\Sigma}^{-1}( \boldsymbol{x} - \boldsymbol{\mu})\right) .
$$

The parameter $\boldsymbol{\mu}$ still gives the mean of the
distribution, even it is now vectorised. The parameter
$\boldsymbol{\Sigma}$ gives the /covariance matrix/ of the distribution.
As in invariance case, when we wish to evaluate the PDF (probability
density function) several times for many different values of the
parameters, the covariance is /not a computationally efficient way/ to
parametrise the distribution, since we need to /invert
$\boldsymbol{\Sigma}$/ to evaluate the PDF. We can instead use a
precise matrix $\boldsymbol{\beta}$: $$
\mathcal{N} (\boldsymbol{x}; \boldsymbol{\mu}, \boldsymbol{\beta^{-1}}) = \sqrt{\frac{\text{det}({\boldsymbol{\beta}})}{(2\pi)^n}} \text{exp} \left(-\frac{1}{2}(\boldsymbol{x} - \boldsymbol{\mu})^{\top} \boldsymbol{\beta}(\boldsymbol{x} - \boldsymbol{\mu}) \right) .
$$

*** Exponential and Laplace Distributions

In deep learning, we often want to have a probability distribution with
a /sharp point at $x=0​$/. To accomplish this, we can use the
/exponential distribution/. $$
p(x;\lambda) = \lambda \boldsymbol{1}_{x \ge 0} \text{exp}(-\lambda x) .
$$

The exponential distribution uses the /indicator function/
$\boldsymbol{1}_{x \ge 0}$ to assign the probability zero to all
negative values of $x$.

A close related distribution that allows us to /place a sharp peak of
probability mass at an /arbitrary/ point $\mu$/ is the /Laplace
distribution/. $$
\text{Laplace}(x;\mu, \gamma) = \frac{1}{2 \gamma} \text{exp} \left( - \frac{|x-\mu|}{\gamma} \right) .
$$

*** Dirac Distribution and Empirical Distribution

In some case, we wish to specify all of the mass in a probability
distribution /clusters around a single point/. This can be accomplished
with /Dirac delta function/, $\delta(x)$: $$
p(x) = \delta(x-\mu) .
$$

The Dirac delta function is defined such that /it is zero-valued
everywhere except 0, yet integers to 1/.

The common use of Dirac delta distribution is as a /component/ of
/empirical distribution/, $$
\hat{p}(\boldsymbol{x}) = \frac{1}{m} \displaystyle \sum_{i=1}^{m} \delta( \boldsymbol{x} - \boldsymbol{x}^{(i)}) .
$$

#+BEGIN_QUOTE
/prior probability/ $a_i = P(c=i)$: expresses the model's beliefs about $c$ /before/ it has observed $\text{x}$.

/posterior probability/ $P(c|\boldsymbol{x})$: is computed /after/ the observation of $\text{x}$.
#+END_QUOTE

** Useful Properties of Common Functions

-  /Logistic sigmoid/ $$
     \sigma(x) = \frac{1}{1 + \exp(-x)} .
     $$ The logistic sigmoid is commonly used to produce the $\phi$
   parameter of a Bernoulli distribution because its range is $(0, 1)$,
   which lies within the valid range of values for the $\phi$ parameter.

#+CAPTION: Logistic Sigmoid
[[https://juicyiter.gitee.io/images/image-20190306165322428.png]]

- /Softplus function/
$$
   \zeta(x) = \log(1 + \exp(x)) .
$$

The soft plus function can be useful for producing the $\beta$ or
$\sigma$ parameter of a normal distribution because its range is
$(0, \infty)$. The name of the softplus function comes from the fact
that it is a smoothed or "softened" version of $x^+ = \max(0, x)$.

#+CAPTION: Softplus Function
[[https://juicyiter.gitee.io/images/image-20190306170340292.png]]

*Useful properties*: $$
   \sigma(x) = \frac{\exp(x)}{\exp(x) + \exp(0)} .
   $$

$$
   \frac{d}{dx} \sigma(x) = \sigma(x)(1 - \sigma(x)) .
   $$

$$
   1 - \sigma(x) = \sigma (x) .
   $$

$$
   \log \sigma(x) = - \zeta(-x) .
   $$

$$
   \frac{d}{dx} \zeta(x) = \sigma(x) .
   $$

$$
   \forall x \in (0, 1), \sigma ^{-1}(x) = \log(\frac{x}{1-x}) .
   $$

$$
   \forall x > 0, \zeta ^{-1}(x) = \log(\exp(x) - 1) .
   $$

$$
   \zeta(x) = \int_{- \infty}^{x} = \sigma(y)dy .
   $$

$$
   \zeta(x) - \zeta(-x) = x.
   $$

** Bayes' Rule

$$
P(\text{x }|\text{ y}) = \frac{P(\text{x})P(\text{y } | \text{ x})}{P(\text{y})}
$$

where $P(\text{y})$ can be computed using $P(\text{x})$:
$P(\text{y}) = \sum_x P(x)P(\text{y } | \; x)$.

* Information Theory

We define the /self-information/ of an event $\text{x}=x$ to be $$
I(x) = -\log P(x) .
$$

#+BEGIN_QUOTE

In deep learning, we always use $\log$ to mean the natural logarithm,
with base $e$. The definition of $I(x)$ is therefore written in units of
/nats/. One nat is the amount of information gained by observing an
event of probability $\frac{1}{e}$.
#+END_QUOTE

Self-information deals only with single outcome. We can quantified the
amount of the uncertainty in an entire probability distribution using
the /Shannon entropy/: $$
H(x) = \mathbb{E}_{\text{x} \sim P}[I(\text{x})] = -\mathbb{E}_{\text{x} \sim P}[\log P(x)] .
$$ also denoted as $H(P)$.

Shannon entropy of distribution describes the expected amount of
information in an event drawn from that distribution. /It gives the
lower bound on the number of bits/ (if the logarithm is based on $2$,
otherwise the units are different) /needed on average to encode symbols
drawn from a distribution/ $P$. Distributions that are nearly
deterministic (where the outcome is nearly certain) have low entropy;
distributions that are closer to uniform have high entropy. When
$\text{x}$ is continuous, the Shannon entropy is known as the
/differential entropy/.

#+BEGIN_QUOTE
Information measured in bits is just a rescaling of information measured
in nats.
#+END_QUOTE

#+CAPTION: The $p$, on the horizontal axis, is the probability of a binary random variable being equal to $1$. The entropy is given by $(p-1) \log (1-p) - p \log (p)$. When the $p$ is near $0$, the distribution is almost deterministic, because the random variable is nearly always $0$. When the $p$ is near $1$, the distribution is almost deterministic, because the random variable is nearly always $1$. When $p = 0.5$, the entropy is maximal, because the distribution is uniform over the two outcomes.
[[https://juicyiter.gitee.io/images/image-20190307153359462.png]]


If you have two separate probability distribution $P(\text{x})$ and
$Q(\text{x})$ over the same variable $\text{x}$, then you can measure
how different these two distributions are using /Kullback Leibler (KL)
divergence/: $$
D_{KL}(P \:|| \: Q) = \mathbb{E}_{\text{x} \sim P}\bigg[ \log \frac{P(x)}{Q(x)} \bigg] = \mathbb{E}_{\text{x} \sim P}\big[ \log P(x) - \log Q(x)\big] .
$$ The $KL$ divergence has many useful properties, most notably that it
is /non-negative/. The $KL$ divergence is $0$ if and only if $P$ and $Q$
are the /same distribution/ in the case of /discrete/ variables, or
equal "almost everywhere" in the case of continuous variables.

#+BEGIN_QUOTE
It's not a true distance measure between these two distributions,
because it is /NOT SYMMETRIC/:
$D_{KL}(P \: || \: Q) \neq D_{KL}(Q \: || \: P)$ for some $P$ and $Q$.
The asymmetry means that there are important consequences to the choice
of whether to use $D_{KL}(P \: || \: Q)$ or $D_{KL}(Q \: || \: P)$.
#+END_QUOTE

#+CAPTION: Corss-entropy
[[https://juicyiter.gitee.io/images/image-20190307162021179.png]]

/Cross-entropy/ is closely related with $KL$ divergence:
$H(P, Q) = H(P) + D_{KL}(P \: || \: Q)$, which is similar to the $KL$
divergence but lacking the term on the left: $$
H(P, Q) = - \mathbb{E}_{\text{x} \sim P} \log Q(x) .
$$ Minimising the cross-entropy with respect to $Q$ is equal to
minimising the $KL$ divergence, as $Q$ does not participate in the
omitted term.

** Structured Probabilistic Models

Suppose we have three random variables: $a, b$ and $c$. Suppose that $a$
influences the value of $b$ and $b$ influences the value of $c$, but
that $a$ and $c$ are independent given $b$. We can represent the
probability distribution of three variables as product of probability
distributions over two variables: $$
p(a, b, c) = p(a)p(b\:|\:a)p(c \: | \: b) .
$$ We describe these kinds of factorisations using graphs: a set of
vertices that may be connected to each other with edges, which we call a
/structured probabilistic model/ or /graphical model/.

-  /Directed/ models contain one factor for every random variable in
   $\text{x}_i$ in the distribution, and that factor consists of the
   conditional distribution over $\text{x}_i$ given the parents of
   $\text{x}_i$, denoted as $Pa_{\mathcal{G}}(\text{x}_i)$: $$
     p(\text{x}) = \displaystyle \prod_i p(\text{x}_iPa_\mathcal{G}(\text{x}_i)) .
     $$

#+CAPTION: Directed Graph
[[https://juicyiter.gitee.io/images/image-20190307202344293.png]]


A directed graphical model over random variables $a, b, c, d$ and $e$.
This graph corresponds to the probability distributions that can be
factored as $$
   p(a, b, c, d, e) = p(a)p(b \: | \: a)p(c \: | \:a, b)p(d\:|\:b)p(e\:|\:c) .
   $$

-  /Undirected/ models, instead, represent factorisations into a set of
   functions, which are usually not probability distributions of any
   kind. Any set of nodes that are all connected to each other is
   $\mathcal{G}$ is called a /clique/. Each clique $\mathcal{C}^{(i)}$
   in an undirected model is associated with a factor
   $\phi^{(i)}(\mathcal{C}^{(i)})$. These factors are /just functions/,
   not probability distributions. The output of each factor must be
   /non-negative/, but there is no constraint that the factor must sum
   or integrate to $1$ like a probability distribution. $$
     p(\mathbf{x}) = \frac{1}{Z} \displaystyle \prod_i \phi^{(i)}(\mathcal{C}^{(i)}) .
     $$ where $Z$ is defined to be the sum of integral over all states
   of the product of the $\phi$ functions, in order to obtain a
   normalised probability distribution.

#+CAPTION: Undirected Graph
[[https://juicyiter.gitee.io/images/image-20190307205320081.png]]

The graph above corresponds to probability distributions that can be
factored as $$
   p(a, b, c, d, e) = \frac{1}{Z} \phi^{(1)}(a, b, c)\phi^{(2)}(b, d)\phi^{(3)}(c, e) .
   $$

* Numerical Computation

-  /Underflow/ occurs when numbers near $0$ are rounded to zero.
-  /Overflow/ occurs when numbers with large magnitude are approximated
   as $\infty$ or $-\infty$.

One example of a function that must be stabilised against underflow and
overflow is the /softmax/ function, which is often used to predict the
probabilities associated with multinoulli distribution. $$
\text{softmax}(\boldsymbol{x})_i = \frac{\exp({x_i})}{\sum_{j=1}^{n} \exp(x_j)} .
$$ If all of the $x_j$ are equal to some constant $c$, analytically, all
the output of the function should be $\frac{1}{n}$. However, this may
not occur if $c$ has a very large magnitude. If $c$ is very negative,
then $\exp(c)$ will underflow, which means the denominator will become
$0$, so the final result is undefined. When $c$ is very large and
negative, the $\exp(c)$ will overflow, again resulting in the expression
as whole being undefined. **Both of these difficulties can be resolved
by evaluating $\text{softmax}(\boldsymbol{z})$ where
$\boldsymbol{z} = \boldsymbol{x} - \max_i x_i$.

-  /Poor Conditioning/

Consider a function
$f(\boldsymbol{x}) = \boldsymbol{A}^{-1}\boldsymbol{x}$. When
$\boldsymbol{A} \in \mathbb{R}^{n \times n}$ has an eigenvalue
decomposition, the /conditional number/ is $$
   \displaystyle \max\limits_{i, j}= \Bigg| \frac{\lambda_i}{\lambda_j} \Bigg| .
   $$ When this number is large, matrix inversion is particularly
sensitive to error in the input.

** Gradient-Based Optimisation

The following terms are all the same thing: /objective function/,
/criterion/, /cost function/, /loss function/ and /error function/. We
often denote the value that minimise or maximise a function with
superscript $*$, like $\boldsymbol{x}^* = \arg \min f(\boldsymbol{x})$

If we know that $f(x - \epsilon \text{ sign} (f'(x))) < f(x)$ for small
enough $\epsilon$. We can thus reduce $f(x)$ by moving $x$ in small
steps with /opposite/ sign of the derivative. This technique is called
/gradient descent/.

#+CAPTION: Gradient Descent
[[https://juicyiter.gitee.io/images/image-20190308102253178.png]]

Points where $f'(x)=0$ are known as /critical points/ or /stationary
points/. That's where the local minima or maxima could lie. For those
critical points that are neither maxima or minima, we call them /saddle
points/.

For functions that contains multiple inputs, we must use the concept of
/partial derivatives/. The gradient of $f​$ is the /vector/ containing
all of the partial derivatives, denoted
$\nabla_{\boldsymbol{x}} f(\boldsymbol{x})​$. Element $i​$ of the
gradient is the partial derivative of $f​$ with respect to $x_i​$,
$\frac{\partial}{\partial x_i}f(\boldsymbol{x})​$. In multi-dimension,
the critical points are where every elements of the gradient is equal to
zero.

The /directional derivative/ in direction $\boldsymbol{u}$ (a unit
vector) is the derivative of the function
$f(\boldsymbol{x} + \alpha \boldsymbol{u})$ with respect to $\alpha$,
evaluated at $\alpha = 0$, which evaluates to
$\boldsymbol{u}^{\top}f(\boldsymbol{x})$. To find the direction in which
$f$ decreases the fastest, we can use the directional derivative: $$
\min\limits_{\boldsymbol{u}, \boldsymbol{u^{\top}u} = 1}\boldsymbol{u^{\top}} \nabla_\boldsymbol{x} f(\boldsymbol{x}) .
$$

$$
= \min\limits_{\boldsymbol{u}, \boldsymbol{u^{\top}u}=1}|| \boldsymbol{u}||_2 ||\nabla_\boldsymbol{x}f(\boldsymbol{x})||_2 \cos \theta .
$$

This is known as the /method of steepest descent/ or /gradient descent/,
which proposes a new point $$
\boldsymbol{x}' = \boldsymbol{x} - \epsilon \nabla_\boldsymbol{x}f(\boldsymbol{x}) .
$$ where $\epsilon$ is the learning rate, a positive scalar determining
the size of the step.

-  /Jacobian matrix/: containing all the partial derivatives. ---
   First-order optimisation.

** Hessian matrix --- Second-order optimisation

$\boldsymbol{H}(f)(\boldsymbol{x})$ the Jacobian matrix of the gradient.
Anywhere that the second partial derivatives are continuous, the
differential operators are commutative, which implies that
$H_{i, j} = H_{j, i}$, so the Hessian matrix is symmetric at this point.
$$
\frac{\partial^2}{\partial x_i \partial x_j}f(\boldsymbol{x}) = \frac{\partial^2}{\partial x_j \partial x_i}f(\boldsymbol{x})
$$ Because the Hessian matrix is real and symmetric, we can decompose it
into a set of real eigenvalues and an orthogonal basis of eigenvectors.
The second derivative in a /specific direction/ represented by a unit
vector $\boldsymbol{d}​$ is given by $\boldsymbol{d^{\top}Hd}​$. When
$d​$ is an eigenvector of $\boldsymbol{H}​$, the second derivative in
that directions /is given by the corresponding eigenvalue/. For other
directions of $\boldsymbol{d}​$, the directional second derivative is a
/weighted average of all the eigenvalues/, with weights between $0​$ and
$1​$, and /eigenvectors that have smaller angle with $\boldsymbol{d}​$
receiving more weight/.

The second-order Taylor series approximation to the function
$f(\boldsymbol{x})$ around the current point $\boldsymbol{x}^{(0)}​$: $$
f(\boldsymbol{x}) \approx f(\boldsymbol{x}^{(0)}) + (\boldsymbol{x}-\boldsymbol{x}^{(0)})^{\top}\boldsymbol{g} + \frac{1}{2}(\boldsymbol{x}- \boldsymbol{x}^{(0)})^{\top} \boldsymbol{H}(\boldsymbol{x}-\boldsymbol{x}^{(0)})
$$ where $\boldsymbol{g}​$ is the gradient and $\boldsymbol{H}​$ is the
Hessian at $\boldsymbol{x}^{(0)}​$. If we use a learning rate
$\epsilon​$, then the new point $\boldsymbol{x}​$ will be given by
$\boldsymbol{x}^{(0)}-\epsilon\boldsymbol{g}​$. Substituting this into
the equation above, we obtain $$
f(\boldsymbol{x}^{(0)}-\epsilon\boldsymbol{g}) \approx f(\boldsymbol{x}^{(0)}) - \epsilon \boldsymbol{g^{\top}g}+\frac{1}{2} \epsilon^2 \boldsymbol{g^\top} \boldsymbol{H}\boldsymbol{g}
$$ There are three therms here in the equation: $\bullet​$ the original
value of the function, $\bullet​$ the expected improvement due to the
slope of the function, and $\bullet​$ the correction we must apply to
account for the curvature of the function. When the
$\boldsymbol{g^{\top}Hg}​$ is /positive/, solving for the optimal step
size that decrease the Taylor series approximation of the function the
most yields $$
\epsilon^* = \frac{\boldsymbol{g^{\top}Hg}}{\boldsymbol{g^{\top}g}}
$$ At a critical point, the eigenvalue of Hessian matrix can be examined
to determine whether the critical point is a local minimum, local
maximum or saddle point.

-  When the Hessian is positive definite, the point is a local minimum.
   Like wise, when the Hessian is negative definite, the point is a
   local maximum.

-  When at least one eigenvalue is positive and at least one eigenvalue
   is negative, we know that $\boldsymbol{x}$ is a local minimum on one
   cross section of $f$ but a local maximum on another cross section.

[[https://juicyiter.gitee.io/images/image-20190308191237380.png]]

When the Hessian has a poor condition number, gradient descent performs
poorly.This is because in one direction, the derivative increases
rapidly, while in another direction, it increases slowly. Gradient
descent does not know this change in the derivative so it does not know
that it needs to explore preferentially in the directions where the
derivative remains negative for longer, making it difficult to choose a
good step size.

[[https://juicyiter.gitee.io/images/IMG_CEEDB9114259-1.jpeg]]

Here the condition number of Hessian is $5$, which means the direction
($[1,1]^{\top}$) of most curvature have five times more curvature than
the direction ($[-1, 1]^{\top}$) of the least curvature.

The problem can be resolve by /Newton's method/, which is based on using second-order Taylor series expansion to approximate <<newton>>
$f(\boldsymbol{x})$ near some point $\boldsymbol{x}^{(0)}$: $$
f(\boldsymbol{x}) \approx f(\boldsymbol{x}^{(0)}) + (\boldsymbol{x}-\boldsymbol{x}^{(0)})^{\top} \nabla_\boldsymbol{x}f(\boldsymbol{x}^{(0)})+ \frac{1}{2} (\boldsymbol{x}-\boldsymbol{x}^{(0)})^{\top} \boldsymbol{H}(f)(\boldsymbol{x^{(0)}})(\boldsymbol{x}-\boldsymbol{x}^{(0)})
$$ Solving the critical point, we obtain: $$
\boldsymbol{x}^* = \boldsymbol{x}^{(0)} - {\boldsymbol{H}(f)f(\boldsymbol{x}^{(0)})}^{-1} \nabla_\boldsymbol{x}f(\boldsymbol{x}^{(0)})
$$

#+BEGIN_QUOTE
When $f$ is a /positive define quadratic/ function, Newton's method
consists of applying [[newton][function]] /once/ to jump to the minimum
of the function directly. When $f$ is not truly quadratic but can be
/locally approximated/ as a positive define quadratic function, Newton's
method consists of applying [[newton][function]] multiple times.

Newton's method is only /approximate/ when the /nearby

critical point is a minimum/, which will be detailedly noted later on.

#+END_QUOTE

A /Lipschitz continuous/ function is a function $f$ whose rate of change
is bounded by a /Lipschitz constant/ $\mathcal{L}$:

$$
\forall \boldsymbol{x}, \forall \boldsymbol{y}, |f(\boldsymbol{x})-f(\boldsymbol{y})| \le \mathcal{L}||\boldsymbol{x}-\boldsymbol{y}||_2
$$

/Convex optimisation/ algorithms are applicable only to convex
functions--- functions for which the Hessian is /positive semidefinite
everywhere, which lack saddle points/ and /all their local minima are
necessarily global minima/.

The /Karush-Kuhn-Tucker/ (KKT) approach provides a very general solution
to constrained optimisation. With the KKT approach, we introduce a new
function called the /generalised Lagrangian/ or /generalised Lagrange
function/.

To define the Lagrangian, we first need to describe $\mathbb{S}$ in
terms of equations and inequalities. $\mathbb{s} = \{ \boldsymbol{x} \: | \: \forall i, g^{(i)}(\boldsymbol{x}) = 0 \text{ and } \forall j, h^{(j)} (\boldsymbol{x}) \le 0 \}$.

The equations involving $g^{(i)}$ are called the /equality constraints/
and the ineuqalities involving $h^{(i)}$ are called /inequality
constraints/.

We introduce new variables $\lambda_i$ and $\alpha_i$ for each
constraint, these are called the /KKT multipliers/. The generalised
Lagrangian is then defined as

$$ L(\boldsymbol{x}, \boldsymbol{\lambda}, \boldsymbol{\alpha}) = f(\boldsymbol{x}) + \displaystyle \sum_i \lambda_i g^{(i)}(\boldsymbol{x}) + \sum_i \alpha_i h^{(j)}(\boldsymbol{x}) $$

/So long as at least one feasible point exists and $f(\boldsymbol{ x})$
is not permitted to have value $\infty$/, then
$$ \min \limits_{\boldsymbol{x}} \max \limits_{\boldsymbol{\lambda}} \max \limits_{\boldsymbol{\alpha, \alpha} \ge 0} L( \boldsymbol{ x}, \boldsymbol{ \lambda}, \boldsymbol{ \alpha}) .$$

has the same optimal objective function value and set of optimal points
$\boldsymbol{ x}$ as
$$ \min \limits_{\boldsymbol{ x} \in \mathbb{S}} f(\boldsymbol{ x}) .$$

This follows because any time the constraints are satisfied,

$$ \max \limits_{\boldsymbol{\lambda}} \max \limits_{\boldsymbol{\alpha, \alpha \ge} 0} L(\boldsymbol{ x, \lambda, \alpha}) = f(\boldsymbol{x}), $$

while any time a constraint is violated,

$$ \max \limits_{\boldsymbol{\lambda}} \max \limits_{\boldsymbol{\alpha, \alpha \ge} 0} L(\boldsymbol{ x, \lambda, \alpha}) = \infty . $$

/These properties guarantee that no infeasible point can be optimal, and
that the optimum within the feasible points is unchanged./

/Karush-Kuhn-Tucker/ (KKT) /conditions/: - The gradient of the
generalised Lagrangian is zero. - All constraints on both
$\boldsymbol{x}$ and the KKT multipliers are satisfied. - The inequality
constraints exhibit "complementary slackness".
$\boldsymbol{\alpha} \odot \boldsymbol{h}(\boldsymbol{x}) = \boldsymbol{0}$.

#+BEGIN_QUOTE

They are necessary conditions, but not always sufficient conditions, for
a point to be optimal.
#+END_QUOTE

Most machine learning algorithms have settings called **hyperparameters** that must be determined external to the learning algorithm itself. Two central approaches to statistics are **frequentist estimators and Bayesian inference**. Most deep learning algorithms are based on an optimisation algorithm called **stochastic gradient descent**.


* Learning Algorithms

What do we mean by learning? A computer program is said to learn from *experience* $E$ with respect to some class of *tasks* $T$ and *performance* measure $P$, if its performance at task $T$, as measured by $P$, improves with experience $E$.
** The Task, $T$
- *Classification*: To solve this task, the learning algorithm is often asked to produce a function $f: \mathbb{R} \to \{ 1, \dots, k \}$. When $y =  f(\boldsymbol{x})$, the model assigns an input described by vector $\boldsymbol{x}$ to a category identified by a numeric code $y$. $f$ *outputs a probability distribution over classes*.

- *Classification with missing inputs*: To solve this task, instead of mapping from a vector a categorical output, the learning algorithm must learn a *set* of functions. One way to define such a large set of functions is *to learn a probability distribution over all of the relevant variables*, then solve the classification task by marginalising out the missing variables. With $n$ input variables, we can obtain $2^n$ different classification functions needed for each possible set of missing inputs, but we *only need to learn a single function describing the /joint distribution/*.

- *Regression*: This type of task is similar to classification, except that the out is different this time: $f:\mathbb{R} \to \mathbb{R}$.

- *Transcription*: In this type of task, the machine learning system is asked to observe a *relatively unstructured* representation of some kind of data and *transcript it into discrete, textual form*. For example, in optical character recognition, the computer program is shown a photograph containing an image of text, the computer program is asked to return its text form of a sequence of characters.

- *Machine translation*: The computer program is asked to convert a sequence of symbols in some language into a sequence of another language.

- *Structured output*: One example is parsing--mapping a natural language sentence into a tree that describes its grammatical structure and tagging nodes of the trees as being verbs, nouns, or adverbs, and so on. The reason the task is called structured output is that the program must output several values that are all tightly inter-related. For example, the words produced by an image captioning program must be a valid sentence.

- *Anomaly detection*: An example is credit card fraud detection for uncharacteristic purchases.

- *Synthesis and sampling*: In this type of task, the machine learning algorithm is asked to generate new examples that are similar to those in the training data. Synthesis via machine learning can be very useful for media applications where it can be expensive or boring for an artist to generate large volumes of content by hand.

- *Imputation of missing values*: Given a new example of $\boldsymbol{x} \in \mathbb{R}^n$, but with some entries $x_i$ of $\boldsymbol{x}$ missing, the algorithm must provide a prediction of the values of the missing entries.

- *Denoising*: The learning algorithm must predict the clean example $\boldsymbol{x}$ from its corrupted version $\tilde{\boldsymbol{x}}$, or more generally predict the conditional probability distribution $p(\boldsymbol{x} \: | \: \tilde{\boldsymbol{x}})$.

- *Density estimation* or *probability mass function estimation*: In this type of task, the algorithm is asked to learn a function $p_{model} : \mathbb{R}^n \to \mathbb{R}$ where $p_{model}(\boldsymbol{x})$ can be interpreted as a probability density function (if $\mathbf{x}$ is continuous) or a probability mass function (if $\mathbf{x}$ is discrete). To do this task well, the algorithm must learn the structure of the data and where the examples cluster tightly and where they unlikely to occur.


** The Performance Measure, $P$
- *Accuracy*: the proportion of examples for which model produces the correct output.

- *Error rate*: the proportion of examples for which model produces the wrong output, which is often referred as $0-1$ loss for discrete inputs. But it doesn't make any sense for continuous inputs. The most common approach for those continuous inputs is to report the *average log-probability* the model assigns to some examples.


** The Experience, $E$
- *Unsupervised learning algorithms* experience a dataset containing many features, then learn useful properties of the structure of this dataset, attempting to implicitly or explicitly learn the probability distribution $p(\mathbf{x})$.

- *Supervised Learning algorithms* experience a dataset containing features, but each feature is also associated with a *label* or *target*. Supervised learning involves observing several examples of a random vector $\mathbf{x}$ and an associated value or vector $\mathbf{y}$, and leaning to predict $\mathbf{y}$ from $\mathbf{x}$, usually by estimating $p(\mathbf{y} \: | \: \mathbf{x})$. Alternatively, we can solve the supervised learning problem of learning $p(y \: | \: \mathbf{x})$ by using traditional unsupervised learning technologies to learn the joint distributions $p(\mathbf{x}, y)$ and inferring
$$ p(y \: | \: \mathbf{x}) = \frac{p(\mathbf{x}, y)}{\sum_{y'} p(\mathbf{x}, y')}. $$

Traditionally, people refer to *regression*, *classification*, and *structured output* as supervised learning. *Density estimation* in support of other tasks is usually considered as unsupervised learning.

#+BEGIN_QUOTE
*Reinforced learning* algorithms interacts with an environment, so there is a *feedback loop* between the learning system and its experience.

#+ATTR_HTML: :target _blank
Check [[http://incompleteideas.net/book/bookdraft2017nov5.pdf][Sutton and Barto's book]] or [[https://www.researchgate.net/publication/216722122_Neuro-Dynamic_Programming][Bertsekas and Tsitsiklis (1996)]] for information about reinforced learning, and [[https://arxiv.org/pdf/1312.5602.pdf][Mnih et al. (2013)]] for the deep learning approach to reinforced learning.
#+END_QUOTE

Most machine learning algorithms simply experience a dataset, which is commonly described with a *design matrix*, whose columns correspond to different features.


** Example: Linear Regression
The goal of the linear regression is to build a system that can take a vector $\boldsymbol{x} \in \mathbb{R}^n$ as input and predict the value of a scalar $y \in \mathbb{R}$ as its output, which is defined to be
$$ \hat{y} = \boldsymbol{w^{\top}x} $$ where $\boldsymbol{w} \in \mathbb{R}^n$ is a vector of *parameters*.

<<mse>>
One way of measuring the performance of the model is to compute the *mean squared error* of the model on the *test set*. if $\hat{\boldsymbol{y}}^{(test)}$ gives the predictions of the model on the test set, then the mean squared error is given by
$$ \mathrm{MSE}_{\mathrm{test}} = \frac{1}{m} \sum \limits_{i} (\hat{\boldsymbol{y}}^{(\mathrm{test})} - \boldsymbol{y}^{(\mathrm{test})})^2_i = \frac{1}{m} || \hat{\boldsymbol{y}}^{(\mathrm{test})} - \boldsymbol{y}^{(\mathrm{test})} ||_2^2.$$

To make a machine learning algorithm, we need to design an algorithm that will improve the weights $\boldsymbol{w}$ in a way that reduces $\mathrm{MSE}_{\mathrm{test}}$ when the algorithm is allowed to gain experience by observing a training set $(\mathbf{X}^{(\mathrm{train})}, \boldsymbol{y}^{(\mathrm{train})})$. One intuitive way of doing this is (which will justify [[http://fa2png.io/media/icons/font-awesome/4-7-0/exclamation-circle/15/0/ff0101_none.png]]) is just to minimise the mean squared error on the training set, $\mathrm{MSE}_{\mathrm{train}}$.

To minimise it, we simply solve for where its gradient is $\boldsymbol{0}$:

$$ \nabla_{\boldsymbol{w}} \mathrm{MSE}_{\mathrm{train}} = 0 $$

$$\implies \nabla_{\boldsymbol{w}} \big( \mathbf{X}^{(\mathrm{train})}\boldsymbol{w} - \boldsymbol{y}^{(\mathrm{train})} \big)^{\top} \big( \mathbf{X}^{(\mathrm{train})}\boldsymbol{w} - \boldsymbol{y}^{(\mathrm{train})} \big) = 0 $$

<<eq_norm>>
$$ \implies \boldsymbol{w} = \bigg( {\mathbf{X}^{(\mathrm{train})}}^{\top} \mathbf{X}^{(\mathrm{train})} \bigg)^{-1} {\mathbf{X}^{(\mathrm{train})}}^{\top} \boldsymbol{y}^{(\mathrm{train})} \tag{*}$$

The system of equations whose solution is given by equation [[eq_norm][*]] is known as the *normal equations*.

* Capacity, Overfitting and Underfitting

The ability to perform well on previously unobserved inputs is called *generalisation*. What separates machine learning from optimisation is that we want the *generalisation error*, also called the *test error*, to be low as well. We typically estimate the generalisation error of a machine learning model by measuring its performance on a *test set* of examples that were *collected separately* from the training set.

*How can we affect performance on the test set when we get to observe only the training set?* The field of *statistical learning theory* provides some answers. If we are allowed to make some assumptions about *how the training and test set are collected*, then we can make some progress.

The train and test data are generated by a probability distribution over datasets called the *data generating progress*. We call that /shared underlying distribution/ the *data generating distribution*, denoted as $p_{\mathrm{data}}$. This probabilistic framework and the i.i.d. assumptions allow us to mathematically study the relationship between training error and the test error.

The expected training error of a randomly selected model is equal to the expected test error of that model, because both expectations are formed using the same dataset sampling progress. But practically the test error is greater or equal to the training error, because we don't fix the parameters ahead of time.

The factors determining how well a machine learning algorithm will perform are its ability to:

1. Make the training error small.
2. Make the gap between the training and test error small.

That brought the two central challenges in machine learning: *underfitting* and *overfitting*.

- Underfitting occurs when the model is not able to obtain a sufficient low error value on the training set.
- Overfitting occurs when the gap between training and test error is too large.

We can control whether a model is more likely to overfit or underfit by altering its *capacity*. One way to do this is by choosing its *hypothesis space*, the set of function that the learning algorithm is allowed to select as being the solution. That is changing the number of inputs features it has, and simultaneously adding new parameters associated with those features.

Another kind of capacity is called *representational capacity*, which specifies which family of function the learning algorithms can choose from when varying the parameters in order to reduce a training objective.
In practice, because of the imperfection of the optimisation algorithm and so on, the learning algorithm's *effective capacity* may be less than the representational capacity of the model family.

#+BEGIN_QUOTE
An interesting principle of parsimony is *Occam's razor*.

This principle states that among competing hypotheses that explain known observation equally well, one should choose the "simplest" one.
#+END_QUOTE

A well-known way of quantifying model capacity is the *Vapnik-Chervonenkis dimension*, or VC dimension, which measures the capacity of a /binary classifier/. The VC dimension is defined as being the largest possible value of $m$ for which there exists a training set of $m$ different $\boldsymbol{x}$ points that the classifier can label arbitrarily.

The most important results in statistical learning theory shows that *the discrepancy* between training error and the generalisation error *grows as the model capacity grows* but *shrinks as the number of training examples increases*. While simpler functions are more likely to generalise (to have small gap between training and test error) we must still choose a *sufficiently complex hypothesis*.

Typically, *training error decrease* until it asymptotes to the minimum possible error value as model capacity increases (assuming the error value has a minimum value) and *generalisation error has a U-shaped curve* as a function of model capacity.

#+CAPTION: At the left end of the graph, training error and generalisation error are both high. This is the *underfitting regime*. Eventually, the size of the gap outweights the decrease in training error, and we enter the *overfitting regime*, where the capacity is too large, above the *optimal capacity*.
[[https://juicyiter.gitee.io/images/image-20190311211352759.png]]

Parametric model learn a function described by a parameter vector whose size is finite and fixed before any data is observed. *Non-parametric* models have no such limitation.

One example of such an algorithm is *nearest neighbour regression*, which simply stores the $\mathbf{X}$ and $\boldsymbol{y}$ from the training set. When asked to classify a test point $\boldsymbol{x}$, the model looks up the nearest entry in the training set and returns the associated regression target. In other words, $\hat{y} = y_i$ where $i = \arg \min ||\mathbf{X}_{i, :} -\boldsymbol{x}||^2_2$. *If the algorithm is allowed to break ties by averaging the $y_i$ values of all $\mathbf{X}_{i, :}$ that are tied for nearest, then this algorithm is able to achieve the minimum possible training error* (which might be greater than zero, if two identical inputs are associated with different outputs) on any regression dataset.

The error incurred by *an oracle* making predictions from the *true distribution* $p(\boldsymbol{x}, y)$ is called the *Bayes error*.

Expected generalisation error can *never increase* as the number of training examples increases. Any fixed *parametric model with less than optimal capacity* will *asymptote* to an error value that exceeds the Bayes error.

#+CAPTION: For each size, there are $40$ ($40$ times on each size) different training sets generated in order to plot error bars showing $95$ percent confidence intervals. For quadratic model, the training error increase as the size of the training set increases (larger datesets harder to fit). Its test error asymptotes to a high value. The test error at *optimal capacity* asymptotes to the Bayes error. The training error can fall below the Bayes error, due to the ability of the training algorithm to *memorise* specific instances of the training set.
[[https://juicyiter.gitee.io/images/image-20190311213831415.png]]

** The No Free Lunch Theorem
The *no free lunch theorem* states that averaged over all possible data generating distributions, every classification algorithm has the same error rate when classifying previously unobserved points. In other words, in some sense, no machine learning algorithm is universally better than any other.
#+BEGIN_QUOTE
Fortunately, these results holds only when we average over /all/ possible data generating distribution.
#+END_QUOTE

** Regularisation
To understanding the regularisation, consider the following example. We can modify the training criterion for linear regression to include *weight decay*. To perform linear regression with weight decay, we minimise a sum comprising both the *mean squared error* on the training and *a criterion* $J(\boldsymbol{w})$ that expresses a *preference* for the weights to have smaller squared $L^2$ norm. Specifically,

$$ J(\boldsymbol{w}) = \mathrm{MSE}_{\mathrm{train}} + \lambda \boldsymbol{w^{\top}w}, $$

where $\lambda$ is a value that chosen ahead of time that *controls the strength of the preference for /smaller weights/*. When $\lambda = 0$, we impose no preference, and *large $\lambda$ forces the weights to become smaller*. /Minimising $J(\boldsymbol{w})$ results in a choice of weights that make a trade-off between fitting the training data and being small/. This gives us solutions that have a smaller slope, or put weight on fewer of the features. ($\lambda$ is fixed. To minimise $J(\boldsymbol{w})$, either minimise MSE or minimise $\boldsymbol{w^{\top}w}$.)

#+CAPTION: Here is a model with degree $9$. (/Left/)With very large $\lambda$, the model is forced to learn a function with no slope at all. (/Center/)With a medium value of $\lambda$, the learning algorithm recovers a curve with the right general shape. Even though the model is capable of representing functions with much more complicated shape, *weight decay* has /encouraged/ it to use a /simpler/ function described by smaller coefficients. (/Right/)With weight decay approaching zero (i.e. using the Moore-Penrose pseudoinverse to solve the underdetermined problem with minimal regularisation), the degree-$9$ polynomial overfits significantly.
[[https://juicyiter.gitee.io/images/image-20190311221517798.png]]


More generally, we can regularise a model that learns a function $f(\boldsymbol{x};\boldsymbol{\theta})$ by adding *a penalty* called a *regulariser* to the cost function. In the case of weight decay, the regulariser is $\Omega(\boldsymbol{w}) = \boldsymbol{w^{\top}w}$.

*Expressing preferences* for on function is a more general way of controlling a model's capacity than including or excluding members from the hypothesis space.

#+BEGIN_QUOTE
/Regularisation is *any modification* we make to a learning algorithm that is intended *to reduce its generalisation* error but not it training error/.
#+END_QUOTE

** Hyperparameters and Validation Sets
Hyperparameters are those settings of machine learning algorithms to control the behaviour of the learning algorithm. The degree of polynomial and $\lambda$ mentioned above are both hyperparameters.

Sometimes a setting is chosen to be a hyperparameters that the learning algorithms does not learn because *it is difficult to optimise*. This applies to all hyperparameters that control model capacity.

#+BEGIN_QUOTE
If learned on the training set, such hyperparameters would always chose the maximum possible model capacity, resulting in overfitting.
#+END_QUOTE

To solve this problem, we need a *validation set* of examples that the training algorithm does not observe. *It's important that the test example are not used in any way to make choice about the model, including its hyperparameters*. Therefore we *always* construct the validation set from the /training/ data. Specifically, we split the training data into two disjoint subsets. One of these subsets is used to learn the parameters (typically $80\%$). The other is the validation set (typically $20\%$), used to *estimate the generalisation error* during or after training, *allowing for the hyperparameters to be updated accordingly*. Since the validation set is used to "train" the hyperparameters, the validation set error will *underestimate* the generalisation error.
*** Cross-Validation
The most common of doing this is the $k$-fold cross-validation procedure, shown below, in which a partition of the dataset is formed by splitting it into $k$ non-overlapping subsets.

#+CAPTION: The test error may be estimated by taking the average test error across $k$ trails. On trial $i$, the $i$-th subset of the data is used as test set and rest of the data is used as the training set.
[[https://juicyiter.gitee.io/images/image-20190312093553594.png]]

#+BEGIN_QUOTE
One problem is that there exist no unbiased estimators of the variance of such average error estimators ([[http://120.52.51.13/www.jmlr.org/papers/volume5/grandvalet04a/grandvalet04a.pdf][Bengio and Grandvalet, 2004]]), but approximations are typically used.
#+END_QUOTE

** Estimators, Bias and Variance
*** Point Estimation
Point estimation is the attempt to provide the *single "best"* prediction of some quantity of interest. Let $\{\boldsymbol{x}^{(1)}, \dots, \boldsymbol{x}^{(m)} \}$ be a set of $m$ independent and identically distributed (i.i.d.) data points. A *point estimator* or *statistic* is /any/ function of the data:

$$ \hat{\boldsymbol{\theta}} = g(\boldsymbol{x}^{(1)}, \dots, \boldsymbol{x}^{(m)}). \tag{} $$

While almost any function thus qualifies as an estimator, a *good estimator* is a function whose output is close to the true underlying $\boldsymbol{\theta}$ (which is *fix but unknown* and a random variable) that generated the training data. (*Any is fine, but good is one.*)

*** Function Estimation
 We assume that there is a function $f(\boldsymbol{x})$ that describes the approximate relationship between $\boldsymbol{y}$ and $\boldsymbol{x}$. For example we may assume that $\boldsymbol{y} = f(\boldsymbol{x}) + \boldsymbol{\epsilon}$, where $\boldsymbol{\epsilon}$ stands for the part of $\boldsymbol{y}$ that is not predictable from $\boldsymbol{x}$. Function estimation is really just the same as estimating a parameter $\boldsymbol{\theta}$; the function estimator $\hat{f}$ is simply a point estimator in function space.

*** Bias
The bias of an estimator is defined as:

$$ \mathrm{bias}(\hat{\boldsymbol{\theta}}_m) = \mathbb{E}(\hat{\boldsymbol{\theta}}_m) - \boldsymbol{\theta} \tag{} $$

An estimator $\hat{\boldsymbol{\theta}}_m$ is said to be *unbiased* if bias($\hat{\boldsymbol{\theta}}_m$) = $\boldsymbol{0}$, which implies that $\mathbb{E}(\hat{\boldsymbol{\theta}}_m) = \boldsymbol{\theta}$. An estimator $\hat{\boldsymbol{\theta}}_m$ is said to be *asymptotically unbiased* if $\lim_{m \to \infty} \mathrm{bias}(\hat{\boldsymbol{\theta}}_m) = \boldsymbol{0}$, which implies that $\lim_{m \to \infty} \mathbb{E}(\hat{\boldsymbol{\theta}}_m) = \boldsymbol{\theta}$.

*** Example: Bernoulli Distribution
<<Bernoulli>>
Consider a set of examples $\{x^{(1)}, \dots, x^{(m)} \}$ that are i.i.d according to Bernoulli distribution with mean $\theta$:

$$ P(x^{(i)}; \theta) = \theta^{x^{(i)}} (1 - \theta)^{1 - x^{(i)}}. $$

To determine whether the estimator $\hat{\theta}_m = \frac{1}{m} \displaystyle \sum_{i=1}^{m} x^{(i)}$ is biased, we can do the following calculation:

$$ \begin{align*} \mathrm{bias}(\hat{\theta}_m) &= \mathbb{E}[\hat{\theta}_m] - \theta \\
&= \mathbb{E} \Bigg[ \frac{1}{m} \displaystyle \sum_{i=1}^{m} x^{(i)} \Bigg] - \theta \\
&= \frac{1}{m} \displaystyle \sum_{i=1}^{m} \sum_{x^{(i)}=0}^{1} \big(x^{(i)} \theta^{x^{(i)}} (1 - \theta)^{1 - x^{(i)}} \big) - \theta \\
&= \frac{1}{m} \displaystyle \sum_{i=1}^{m} (\theta) - \theta \\
&= \theta - \theta = 0 \tag{} \end{align*}$$

Since $\mathrm{bias}(\hat{\theta}) = 0$, we say that the estimator $\hat{\theta}$ is unbiased.

*** Example: Gaussian Distribution Estimator of the Mean

Recall that the Gaussian probability density function is given by

$$ p(x^{(i)}; \mu, \sigma^2) = \frac{1}{\sqrt{2 \pi \sigma^2}} \exp \Bigg( - \frac{1}{2} \frac{(x^{(i)} - \mu)^2}{\sigma^2} \Bigg). $$

Consider a set of examples $\{x^{(1)}, \dots, x^{(m)} \}$ that are i.i.d according to Gaussian distribution with *sample mean*:

$$ \hat{\mu}_m = \frac{1}{m} \displaystyle \sum_{i=1}^{m} x^{(i)} $$

The bias of the sample mean is:

$$ \begin{align*}
\mathrm{bias}(\hat{\mu}_m) &= \mathbb{E}[\hat{\mu}_m] - \mu \\
&= \mathbb{E} \Bigg[ \frac{1}{m} \displaystyle \sum_{i=1}^{m} x^{(i)} \Bigg] - \mu \\
&= \Bigg( \frac{1}{m} \displaystyle \sum_{i=1}^{m} \mathbb{E} \big[ x^{(i)} \big] \Bigg) - \mu \\
&= \Bigg( \frac{1}{m} \displaystyle \sum_{i=1}^{m} \mu \Bigg) - \mu \\
&= \mu - \mu = 0 \tag{}
\end{align*} $$

Thus the sample mean is an unbiased estimator of Gaussian mean parameter.

*** Example: Estimators of the Variance of a Gaussian Distribution

The *sample variance* $\hat{\sigma}^2_m = \frac{1}{m} \displaystyle \sum_{i=1}^{m} \big( x^{(i)} - \hat{\mu}_m \big)^2$ , where $\hat{\mu}_m$ is the sample mean, defined above. By evaluating:

\begin{align*} \mathbb{E}[\hat{\sigma}^2_m]
&= \mathbb{E} \Bigg[ \frac{1}{m} \displaystyle \sum_{i=1}^{m} \big( x^{(i)} - \hat{\mu}_m \big)^2 \Bigg] \\
&= \frac{m-1}{m} \sigma^2 \tag{}
\end{align*}

we can therefore conclude that the *sample variance is a biased estimator*.

The *unbiased sample variance* estimator is:

$$
\tilde{\sigma}_m^2 = \frac{1}{m-1} \displaystyle \sum_{i=1}^{m} \big( x^{(i)} - \hat{\mu}_m \big)^2.
$$

*** Variance and Standard Error
The *variance* is denoted as $\mathrm{Var}(\hat{\theta})$, and the squared root of the variance is called the *standard error*, denoted as $\mathrm{SE}(\hat{\theta})$.

The standard error of the mean is given by

$$
\mathrm{SE}(\hat{\mu}_m) = \sqrt{ \mathrm{Var} \Bigg[ \frac{1}{m} \displaystyle \sum_{i=1}^{m} x^{(i)} \Bigg]} = \frac{\sigma}{\sqrt{m}}
$$

Unfortunately, neither the squared root of the sample variance of the squared root of the unbiased estimator of the variance provide an unbiased estimate of the standard deviation. Both approaches tend to *underestimate* the true standard deviation, but still used in practice.

We often estimate the generalisation error by computing the sample mean of the error on the test set. The number of examples in the test set determines the accuracy of this estimate. Taking advantage of *central limit theorem*, which tells us that the mean will approximately distributed with normal distribution, we can use the standard error to compute the probability that the *true expectation falls in any chosen interval*. For example, the $95\%$ confidence interval centred on the mean $\hat{\mu}_m$ is $(\hat{\mu}_m - 1.96\mathrm{SE}(\hat{\mu}_m), \hat{\mu}_m + 1.96\mathrm{SE}(\hat{\mu}_m))$, under the normal distribution with mean $\hat{\mu}_m$ and variance $\mathrm{SE}(\hat{\mu}_m)^2$.

In machine learning experiments, it's common to say that algorithm $A$ is better than algorithm $B$ if the *upper bound* of the $95\%$ confidence interval for the error of algorithm $A$ is less than the *lower bound* of the $95\%$ confidence interval for the error of algorithm $B$.

*** Example: Bernoulli Distribution
Consider the same example mentioned [[Bernoulli][here]]. This time we are interested in computing the variance of the estimator $\hat{\theta}_m = \frac{1}{m} \sum_{i=1}^{m} x^{(i)}$.

\begin{align*} \mathrm{Var}\big( \hat{\theta}_m \big)
&= \mathrm{Var} \Bigg( \frac{1}{m} \displaystyle \sum_{i=1}^{m} x^{(i)} \Bigg) \\
&= \frac{1}{m^2} \sum_{i=1}^{m} \mathrm{Var} \big( x^{(i)} \big) \\
&= \frac{1}{m^2} \sum_{i=1}^{m} \theta (1 - \theta) \\
&= \frac{1}{m^2} m \theta (1 - \theta) \\
&= \frac{1}{m} \theta (1 - \theta) \tag{}
\end{align*}

*** Trading off Bias and Variance to Minimise Mean Squared Error
*Bias* measures the expected deviation from the true value of the function or parameter. *Variance* on the other hand, provides a measure of the deviation from the expected estimator value that any particular sampling of the data is likely to cause.

The most common way to negotiate this trade-off is to use *cross-validation*. Empirically, cross-validation is highly successful on many real-world tasks. Alternatively, we can also compare the [[mse][mean squared error]] (MSE) of the estimates:

\begin{align*}
\mathrm{MSE} &= \mathbb{E}[(\hat{\theta}_m - \theta)^2] \\
&= \mathrm{Bias}(\hat{\theta}_m)^2 + \mathrm{Var}(\hat{\theta}_m) \tag{}
\end{align*}

The MSE measures the *overall expected deviation* --in a squared error sense -- between the estimator and the true value of the parameter $\theta$. Desirable estimators are those with small MSE and these are estimators that manage to keep both their bias and variance somewhat in check.

#+CAPTION: Increasing capacity tends to increase variance and decrease bias.
[[https://juicyiter.gitee.io/images/image-20190312135148020.png]]

*** Consistency

$$ \mathrm{plim}_{m \to \infty} \hat{\theta}_m = \theta $$

The symbol $\mathrm{plim}$ indicates that convergence in probability, meaning that for any $\epsilon > 0$, $P(|\hat{\theta}_m - \theta| > \epsilon) \to 0$ as $m \to \infty$. The condition described above is known as *consistency*, which is sometimes referred to as weak consistency. While strong consistency refers to the *almost sure* convergence of $\hat{\theta}$ to $\theta$. Almost sure convergence occurs when $p(\lim_{m \to \infty} \mathbf{x}^{(m)} = \boldsymbol{x}) = 1$.

#+BEGIN_QUOTE
Consider estimating consisting of $m$ samples: $\{ x^{(1)}, \dots, x^{(m)} \}$. /We could use the first sample $x^{(1)}$ of the dataset as an unbiased estimator: $\hat{\theta} = x^{(1)}$./ In that case, $\mathbb{E}(\hat{\theta}_m) = \theta$ so the estimator is unbiased no matter how many data points are seen. This, of course, implies that *the estimate is asymptotically unbiased*. However, this is not a consistent estimator as it is /not/ the case that $\hat{\theta}_m \to \theta$ as $m \to \infty$.
#+END_QUOTE
** Maximum Likelihood Estimation
Consider a set of $m$ examples $\mathbb{X} = \{ \boldsymbol{x}^{(1)}, \dots, \boldsymbol{x}^{(m)} \}$ drawn independently from the *true but unknown* data generating distribution $p_{\mathrm{data}} (\mathbf{x})$. Let $p_{\mathrm{model}}(\mathbf{x}; \boldsymbol{\theta})$ be a parametric family of probability distributions over the same space indexed by $\boldsymbol{\theta}$. In other words, $p_{\mathrm{model}}(\mathbf{x}; \boldsymbol{\theta})$ maps any configuration $\boldsymbol{x}$ to a real number estimating the true probability $p_{\mathrm{data}}(\boldsymbol{x})$.

The maximum likelihood estimator for $\boldsymbol{\theta}$ is then defined as

<<max_here>>
\begin{align*}
\boldsymbol{\theta}_{\mathrm{ML}}
&= \underset{\boldsymbol{\theta}}{\arg \max} p_{\mathrm{model}} (\mathbb{X}; \boldsymbol{\theta}) \\
&= \underset{\boldsymbol{\theta}}{\arg \max} \displaystyle \prod_{i=1}^{m} p_{\mathrm{data}} (\boldsymbol{x}^{(i)}; \boldsymbol{\theta}) \\
&= \underset{\boldsymbol{\theta}}{\arg \max} \sum_{i=1}^{m} \log p_{\mathrm{data}} (\boldsymbol{x}^{(i)}; \boldsymbol{\theta}) \\
&= \underset{\boldsymbol{\theta}}{\arg \max} \mathbb{E}_{\mathbf{x} \sim \hat{p}_{\mathrm{data}}} \log p_{\mathrm{model}} (\boldsymbol{x}; \boldsymbol{\theta}).
\end{align*}

One way to interpret maximum likelihood estimation is to view it as *minimising the dissimilarity* between the empirical distribution $\hat{p}_{\mathrm{data}}$ defined by the training set and the model distribution, *with the degree of dissimilarity the degree of dissimilarity between the two measured by the KL divergence*. The KL divergence is given by

$$ D_{\mathrm{KL}} (\hat{p}_{\mathrm{data}} || p_{\mathrm{model}}) = \mathbb{E}_{\mathbf{x} \sim \hat{p}_{\mathrm{data}}} [\log \hat{p}_{\mathrm{data}} (\boldsymbol{x}) - \log p_{\mathrm{model}} (\boldsymbol{x})].  $$

The term on the left is a function only of the data generating process, not the model. This means when we train the model to minimise the KL divergence, we only need to minimise $-\mathbb{E}_{\mathrm{x} \sim \hat{p}_{\mathrm{data}}} [\log p_{\mathrm{model}} (\boldsymbol{x})]$, which is of course the same as the maximisation in [[max_here][this]] equation.

Minimising this KL divergence corresponds exactly to minimising the *cross-entropy* between the distributions.

#+BEGIN_QUOTE
Any loss consisting of a *negative* log-likelihood is a cross-entropy between the empirical distribution defined by the training set and probability distribution defined by model.

For example, MSE is the cross-entropy between the empirical distribution and a Gaussian model.
#+END_QUOTE

We can see maximum likelihood as an *attempt to make the model distribution match the empirical distribution $\hat{p}_{\mathrm{data}}$ (we have no direct access to the true distribution $p_{\mathrm{data}}$.

While the optimal $\boldsymbol{\theta}$ is the same regardless of whether we are maximising the likelihood or minimising the KL divergence, *the values of the object function are different*. In software, we often phrase both as *minimising a cost function*. Maximum likelihood thus become minimisation of *negative log-likelihood* (NLL), or equivalently, minimisation of the cross-entropy.

*** Conditional Log-Likelihood and Mean Squared Error

The conditional maximum likelihood estimator is

$$ \boldsymbol{\theta}_{\mathrm{ML}} = \underset{\boldsymbol{\theta}}{\arg \max} P(\boldsymbol{Y} \: | \: \boldsymbol{X} ; \boldsymbol{\theta}). $$

If the examples are assumed to be i.i.d., then this can be decomposed into
<<nnl>>
$$ \boldsymbol{\theta}_{\mathrm{ML}} = \underset{\boldsymbol{\theta}}{\arg \max} \displaystyle \sum_{i=1}^{m} \log P(\boldsymbol{y}^{(i)} \: | \: \boldsymbol{x}^{(i)} ; \boldsymbol{\theta}). $$

*** Example: Linear Regression as Maximum Likelihood
Instead of producing a single prediction $\hat{y}$, we now think of the model as *producing a conditional distribution* $p(y \: | \: \boldsymbol{x})$. The goal of the learning algorithm is now to fit the distribution $p(y \: | \: \boldsymbol{x})$ to all of those different $y$ values that all compatible with $\boldsymbol{x}$. We define $p(y \: | \: \boldsymbol{x}) = \mathcal{N} (y:\hat{y}(\boldsymbol{x};\boldsymbol{w}), \sigma^2)$, where the function $\hat{y}(\boldsymbol{x}; \boldsymbol{w})$ gives the prediction of the mean of the Gaussian. Since the examples are assumed to be i.i.d., the conditional [[nnl][log-likelihood]] is given by

$$\displaystyle \sum_{i=1}^{m} \log P(\boldsymbol{y}^{(i)} \: | \: \boldsymbol{x}^{(i)} ; \boldsymbol{\theta}) = -m \log \sigma - \frac{m}{2} \log(2 \pi) - \sum_{i=1}^{m} \frac{||\hat{y}^{(i)} - y^{(i)}||^2}{2 \sigma^2},$$

where $\hat{y}^{(i)}$ is the output of the linear regression on the $i$-th input $\boldsymbol{x}^{(i)}$ and $m$ is the number of the training examples. Compare to [[mse][MSE]] we can see that maximising the log-likelihood with respect to $\boldsymbol{w}$ yields the same estimate of the parameters $\boldsymbol{w}$ as does minimising the mean squared error. The two criteria have different values but the same locations of the optimum.

*** TODO Properties of Maximum Likelihood
